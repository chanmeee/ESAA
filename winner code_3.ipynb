{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 승자의 지혜 - 8등 소스코드 분석 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train 데이터의 크기가 2GB 정도로 코드를 돌리는 시간이 많이 소요되어 train 데이터 중 100만 개만 사용 (코드 by 정현)\n",
    "#### ** 파일을 다운받은 경우, 다운받은 'train_ver2_trunc.csv' 파일이 있는 곳으로 경로 위치를 설정해주세요. 파일이 필요한 경우, cell type을 code로 바꾸어 아래에 있는 코드를 실행시켜 주세요 \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "trn = pd.read_csv('../../data/train_ver2.csv') # 다운 받은 파일이 해당 경로에 있는지 확인\n",
    "trn = trn.sort_values(by=['ncodpers'], axis=0).iloc[1:1000000,]\n",
    "trn.drop(trn.columns[[0]], axis='columns')\n",
    "\n",
    "trn.to_csv('..\\\\..\\\\data\\\\train_ver2_trunc.csv', # 저장할 경로 지정 \n",
    "           sep=',', na_rep='NaN',\n",
    "           index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. 데이터 준비 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터와 테스트 데이터를 하나의 데이터로 통합하는 코드이다.\n",
    "def clean_data(fi, fo, header, suffix):\n",
    "    \n",
    "    # fi : 훈련/테스트 데이터를 읽어오는 file iterator\n",
    "    # fo : 통합되는 데이터가 write되는 경로\n",
    "    # header : 데이터에 header 줄을 추가할 것인지를 결정하는 boolean\n",
    "    # suffix : 훈련 데이터에는 48개의 변수가 있고, 테스트 데이터에는 24개의 변수만 있다. suffix로 부족한 테스트 데이터 24개분을 공백으로 채운다.\n",
    "\n",
    "    # csv의 첫줄, 즉 header를 읽어온다\n",
    "    head = fi.readline().strip(\"\\n\").split(\",\")\n",
    "    head = [h.strip('\"') for h in head]\n",
    "\n",
    "    # ‘nomprov’ 변수의 위치를 ip에 저장한다\n",
    "    for i, h in enumerate(head):\n",
    "        if h == \"nomprov\":\n",
    "            ip = i\n",
    "\n",
    "    # header가 True 일 경우에는, 저장할 파일의 header를 write한다\n",
    "    if header:\n",
    "        fo.write(\"%s\\n\" % \",\".join(head))\n",
    "\n",
    "    # n은 읽어온 변수의 개수를 의미한다 (훈련 데이터 : 48, 테스트 데이터 : 24)\n",
    "    n = len(head)\n",
    "    for line in fi:\n",
    "        # 파일의 내용을 한줄 씩 읽어와서, 줄바꿈(\\n)과 ‘,’으로 분리한다\n",
    "        fields = line.strip(\"\\n\").split(\",\")\n",
    "\n",
    "        # ‘nomprov’변수에 ‘,’을 포함하는 데이터가 존재한다. ‘,’으로 분리된 데이터를 다시 조합한다\n",
    "        if len(fields) > n:\n",
    "            prov = fields[ip] + fields[ip+1]\n",
    "            del fields[ip]\n",
    "            fields[ip] = prov\n",
    "\n",
    "        # 데이터 개수가 n개와 동일한지 확인하고, 파일에 write한다. 테스트 데이터의 경우, suffix는24개의 공백이다\n",
    "        assert len(fields) == n\n",
    "        fields = [field.strip() for field in fields]\n",
    "        fo.write(\"%s%s\\n\" % (\",\".join(fields), suffix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하나의 데이터로 통합하는 코드를 실행한다. 먼저 훈련 데이터를 write하고, 그 다음으로 테스트 데이터를 write한다. 이제부터 하나의 dataframe만을 다루며 데이터 전처리를 진행한다.\n",
    "with open(\"../input/8th.clean.all.csv\", \"w\") as f:\n",
    "    clean_data(open(\"../../data/train_ver2_trunc.csv\"), f, True, \"\") # 경로 확인 \n",
    "    comma24 = \"\".join([\",\" for i in range(24)])\n",
    "    clean_data(open(\"../../data/test_ver2.csv\"), f, False, comma24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lightgbm 모듈 설치\n",
    "#### ** 파일이 필요할 경우, cell type을 code로 바꾸어 실행시켜 주세요 \n",
    "\n",
    "! pip install lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM\n",
    "\n",
    "Tree 기반의 러닝 알고리즘을 사용한 gradient boosting framework 입니다. 아래와 같은 장점이 있습니다.\n",
    "\n",
    "- Faster training speed and higher efficiency. (빠른 훈련 속도와 높은 효율)\n",
    "- Lower memory usage. (적은 메모리 사용)\n",
    "- Better accuracy. (높은 정확도)\n",
    "- Support of parallel and GPU learning. (병렬 처리와 GPU 러닝 지원)\n",
    "- Capable of handling large-scale data. (큰 scale 데이터 다룰 수 있음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math    \n",
    "import io    \n",
    "\n",
    "# 파일 압축 용도\n",
    "import gzip    \n",
    "import pickle    \n",
    "import zlib    \n",
    "\n",
    "# 데이터, 배열을 다루기 위한 기본 라이브러리\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# 범주형 데이터를 수치형으로 변환하기 위한 전처리 도구\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import engines\n",
    "from utils import *\n",
    "\n",
    "np.random.seed(2016)\n",
    "transformers = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chanmi Yoo\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (11,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# “데이터 준비”에서 통합한 데이터를 읽어온다\n",
    "fname = \"../input/8th.clean.all.csv\"\n",
    "train_df = pd.read_csv(fname) # dtype=dtypes 삭제 \n",
    "\n",
    "# products는 util.py에서 정의한 24개의 금융 제품이름이다\n",
    "# 결측값을 0.0으로 대체하고, 정수형으로 변환한다\n",
    "for prod in products:\n",
    "    train_df[prod] = train_df[prod].fillna(0.0).astype(np.int8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(df, features, name):\n",
    "    # 데이터 프레임 df의 변수 name의 값을 모두 string으로 변환한다\n",
    "    df[name] = df[name].astype('str')\n",
    "    # 이미, label_encode 했던 변수일 경우, transformer[name]에 있는 LabelEncoder()를 재활용한다\n",
    "    if name in transformers:\n",
    "        df[name] = transformers[name].transform(df[name])\n",
    "    # 처음 보는 변수일 경우, transformer에 LabelEncoder()를 저장하고, .fit_transform() 함수로 label encoding을 수행한다\n",
    "    else: # train\n",
    "        transformers[name] = LabelEncoder()\n",
    "        df[name] = transformers[name].fit_transform(df[name])\n",
    "    # label encoding한 변수는 features 리스트에 추가한다\n",
    "    features.append(name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_top(s, count=100, dtype=np.int8):\n",
    "    # 모든 고유값에 대한 빈도를 계산한다\n",
    "    uniqs, freqs = np.unique(s, return_counts=True)\n",
    "    # 빈도 Top 100을 추출한다\n",
    "    top = sorted(zip(uniqs,freqs), key=lambda vk: vk[1], reverse = True)[:count]\n",
    "    # { 기존 데이터 : 순위 } 를 나타내는 dict()를 생성한다\n",
    "    top_map = {uf[0]: l+1 for uf, l in zip(top, range(len(top)))}\n",
    "    # 고빈도 100개의 데이터는 순위로 대체하고, 그 외는 0으로 대체한다\n",
    "    return s.map(lambda x: top_map.get(x, 0)).astype(dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날짜 데이터를 월 단위 숫자로 변환하는 함수\n",
    "def date_to_float(str_date):\n",
    "    if str_date.__class__ is float and math.isnan(str_date) or str_date == \"\":\n",
    "        return np.nan\n",
    "    Y, M, D = [int(a) for a in str_date.strip().split(\"-\")]\n",
    "    float_date = float(Y) * 12 + float(M)\n",
    "    return float_date\n",
    "\n",
    "# 날짜 데이터를 월 단위 숫자로 변환하되 1~18 사이로 제한하는 함수\n",
    "def date_to_int(str_date):\n",
    "    Y, M, D = [int(a) for a in str_date.strip().split(\"-\")] # \"2016-05-28\"\n",
    "    int_date = (int(Y) - 2015) * 12 + int(M)\n",
    "    assert 1 <= int_date <= 12 + 6\n",
    "    return int_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_one_hot(df, features, name, names, dtype=np.int8, check=False):\n",
    "    for n, val in names.items():\n",
    "        # 신규 변수명을 “변수명_숫자”로 지정한다\n",
    "        new_name = \"%s_%s\" % (name, n)\n",
    "        # 기존 변수에서 해당 고유값을 가지면 1, 그 외는 0인 이진 변수를 생성한다\n",
    "        df[new_name] = df[name].map(lambda x: 1 if x == val else 0).astype(dtype)\n",
    "        features.append(new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transforms(train_df):\n",
    "    # 학습에 사용할 변수를 저장할 features 리스트를 생성한다\n",
    "    features = []\n",
    "\n",
    "    # 두 변수를 label_encode() 한다\n",
    "    label_encode(train_df, features, \"canal_entrada\")\n",
    "    label_encode(train_df, features, \"pais_residencia\")\n",
    "\n",
    "    # age의 결측값을 0.0으로 대체하고, 모든 값을 정수로 변환한다.\n",
    "    train_df[\"age\"] = train_df[\"age\"].fillna(0.0).astype(np.int16)\n",
    "    features.append(\"age\")\n",
    "\n",
    "    # renta의 결측값을 1.0으로 대체하고, log를 씌워 분포를 변형한다\n",
    "    train_df[\"renta\"].fillna(1.0, inplace=True)\n",
    "    train_df[\"renta\"] = train_df[\"renta\"].map(math.log)\n",
    "    features.append(\"renta\")\n",
    "\n",
    "    # 고빈도 100개의 순위를 추출한다\n",
    "    train_df[\"renta_top\"] = encode_top(train_df[\"renta\"])\n",
    "    features.append(\"renta_top\")\n",
    "\n",
    "    # 결측값 혹은 음수를 0으로 대체하고, 나머지 값은 +1.0 은 한 후에, 정수로 변환한다\n",
    "    train_df[\"antiguedad\"] = train_df[\"antiguedad\"].map(lambda x: 0.0 if x < 0 or math.isnan(x) else x+1.0).astype(np.int16)\n",
    "    features.append(\"antiguedad\")\n",
    "\n",
    "    # 결측값을 0.0으로 대체하고, 정수로 변환한다\n",
    "    train_df[\"tipodom\"] = train_df[\"tipodom\"].fillna(0.0).astype(np.int8)\n",
    "    features.append(\"tipodom\")\n",
    "    train_df[\"cod_prov\"] = train_df[\"cod_prov\"].fillna(0.0).astype(np.int8)\n",
    "    features.append(\"cod_prov\")\n",
    "\n",
    "    # fecha_dato에서 월/년도를 추출하여 정수값으로 변환한다\n",
    "    train_df[\"fecha_dato_month\"] = train_df[\"fecha_dato\"].map(lambda x: int(x.split(\"-\")[1])).astype(np.int8)\n",
    "    features.append(\"fecha_dato_month\")\n",
    "    train_df[\"fecha_dato_year\"] = train_df[\"fecha_dato\"].map(lambda x: float(x.split(\"-\")[0])).astype(np.int16)\n",
    "    features.append(\"fecha_dato_year\")\n",
    "\n",
    "    # 결측값을 0.0으로 대체하고, fecha_alta에서 월/년도를 추출하여 정수값으로 변환한다\n",
    "    # x.__class__는 결측값일 경우 float를 반환하기 때문에, 결측값 탐지용으로 사용하고 있다\n",
    "    train_df[\"fecha_alta_month\"] = train_df[\"fecha_alta\"].map(lambda x: 0.0 if x.__class__ is float else float(x.split(\"-\")[1])).astype(np.int8)\n",
    "    features.append(\"fecha_alta_month\")\n",
    "    train_df[\"fecha_alta_year\"] = train_df[\"fecha_alta\"].map(lambda x: 0.0 if x.__class__ is float else float(x.split(\"-\")[0])).astype(np.int16)\n",
    "    features.append(\"fecha_alta_year\")\n",
    "\n",
    "    # 날짜 데이터를 월 기준 수치형 변수로 변환한다\n",
    "    train_df[\"fecha_dato_float\"] = train_df[\"fecha_dato\"].map(date_to_float)\n",
    "    train_df[\"fecha_alta_float\"] = train_df[\"fecha_alta\"].map(date_to_float)\n",
    "\n",
    "    # fecha_dato 와 fecha_alto의 월 기준 수치형 변수의 차이값을 파생 변수로 생성한다\n",
    "    train_df[\"dato_minus_alta\"] = train_df[\"fecha_dato_float\"] - train_df[\"fecha_alta_float\"]\n",
    "    features.append(\"dato_minus_alta\")\n",
    "\n",
    "    # 날짜 데이터를 월 기준 수치형 변수로 변환한다 (1 ~ 18 사이 값으로 제한)\n",
    "    train_df[\"int_date\"] = train_df[\"fecha_dato\"].map(date_to_int).astype(np.int8)\n",
    "\n",
    "    # 자체 개발한 one-hot-encoding을 수행한다\n",
    "    custom_one_hot(train_df, features, \"indresi\", {\"n\":\"N\"})\n",
    "    custom_one_hot(train_df, features, \"indext\", {\"s\":\"S\"})\n",
    "    custom_one_hot(train_df, features, \"conyuemp\", {\"n\":\"N\"})\n",
    "    custom_one_hot(train_df, features, \"sexo\", {\"h\":\"H\", \"v\":\"V\"})\n",
    "    custom_one_hot(train_df, features, \"ind_empleado\", {\"a\":\"A\", \"b\":\"B\", \"f\":\"F\", \"n\":\"N\"})\n",
    "    custom_one_hot(train_df, features, \"ind_nuevo\", {\"new\":1})\n",
    "    custom_one_hot(train_df, features, \"segmento\", {\"top\":\"01 - TOP\", \"particulares\":\"02 - PARTICULARES\", \"universitario\":\"03 - UNIVERSITARIO\"})\n",
    "    custom_one_hot(train_df, features, \"indfall\", {\"s\":\"S\"})\n",
    "    custom_one_hot(train_df, features, \"tiprel_1mes\", {\"a\":\"A\", \"i\":\"I\", \"p\":\"P\", \"r\":\"R\"}, check=True)\n",
    "    custom_one_hot(train_df, features, \"indrel\", {\"1\":1, \"99\":99})\n",
    "\n",
    "    # 결측값을 0.0으로 대체하고, 그 외는 +1.0을 더하고, 정수로 변환한다\n",
    "    train_df[\"ind_actividad_cliente\"] = train_df[\"ind_actividad_cliente\"].map(lambda x: 0.0 if math.isnan(x) else x+1.0).astype(np.int8)\n",
    "    features.append(\"ind_actividad_cliente\")\n",
    "\n",
    "    # 결측값을 0.0으로 대체하고, “P”를 5로 대체하고, 정수로 변환한다\n",
    "    train_df[\"indrel_1mes\"] = train_df[\"indrel_1mes\"].map(lambda x: 5.0 if x == \"P\" else x).astype(float).fillna(0.0).astype(np.int8)\n",
    "    features.append(\"indrel_1mes\")\n",
    "    \n",
    "    # 데이터 전처리/피쳐 엔지니어링이 1차적으로 완료된 데이터 프레임 train_df와 학습에 사용할 변수 리스트 features를 tuple 형태로 반환한다\n",
    "    return train_df, tuple(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 48개의 변수마다 전처리/피처 엔지니어링을 적용한다\n",
    "train_df, features = apply_transforms(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prev_df(train_df, step):\n",
    "    # 새로운 데이터 프레임에 ncodpers를 추가하고, int_date를 step만큼 이동시킨 값을 넣는다\n",
    "    prev_df = pd.DataFrame()\n",
    "    prev_df[\"ncodpers\"] = train_df[\"ncodpers\"]\n",
    "    prev_df[\"int_date\"] = train_df[\"int_date\"].map(lambda x: x+step).astype(np.int8)\n",
    "\n",
    "    # “변수명_prev1” 형태의 lag 변수를 생성한다\n",
    "    prod_features = [\"%s_prev%s\" % (prod, step) for prod in products]\n",
    "    for prod, prev in zip(products, prod_features):\n",
    "        prev_df[prev] = train_df[prod]\n",
    "\n",
    "    return prev_df, tuple(prod_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_with_prev(df, prev_df, how):\n",
    "    # pandas merge 함수를 통해 join\n",
    "    df = df.merge(prev_df, on=[\"ncodpers\", \"int_date\"], how=how)\n",
    "    # 24개 금융 변수를 소수형으로 변환한다\n",
    "    for f in set(prev_df.columns.values.tolist()) - set([\"ncodpers\", \"int_date\"]):\n",
    "        df[f] = df[f].astype(np.float16)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_dfs = []\n",
    "prod_features = None\n",
    "\n",
    "use_features = frozenset([1,2])\n",
    "# 1 ~ 5까지의 step에 대하여 make_prev_df()를 통해 lag-n 데이터를 생성한다\n",
    "for step in range(1,6):\n",
    "    prev1_train_df, prod1_features = make_prev_df(train_df, step)\n",
    "    # 생성한 lag 데이터는 prev_dfs 리스트에 저장한다\n",
    "    prev_dfs.append(prev1_train_df)\n",
    "    # features에는 lag-1,2만 추가한다\n",
    "    if step in use_features:\n",
    "        features += prod1_features\n",
    "    # prod_features에는 lag-1의 변수명만 저장한다\n",
    "    if step == 1:\n",
    "        prod_features = prod1_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, prev_df in enumerate(prev_dfs):\n",
    "    how = \"inner\" if i == 0 else \"left\"\n",
    "    train_df = join_with_prev(train_df, prev_df, how=how)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chanmi Yoo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "C:\\Users\\Chanmi Yoo\\Anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1628: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  keepdims=keepdims)\n",
      "C:\\Users\\Chanmi Yoo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\Chanmi Yoo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: RuntimeWarning: All-NaN slice encountered\n",
      "C:\\Users\\Chanmi Yoo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: RuntimeWarning: All-NaN slice encountered\n"
     ]
    }
   ],
   "source": [
    "# 24개의 금융 변수에 대해서 for loop을 돈다\n",
    "for prod in products:\n",
    "    # [1~3], [1~5], [2~5] 의 3개 구간에 대해서 표준편차를 구한다\n",
    "    for begin, end in [(1,3),(1,5),(2,5)]:\n",
    "        prods = [\"%s_prev%s\" % (prod, i) for i in range(begin,end+1)]\n",
    "        mp_df = train_df.as_matrix(columns=prods)\n",
    "        stdf = \"%s_std_%s_%s\" % (prod,begin,end)\n",
    "\n",
    "        # np.nanstd로 표준편차를 구하고, features에 신규 파생 변수 이름을 추가한다\n",
    "        train_df[stdf] = np.nanstd(mp_df, axis=1)\n",
    "        features += (stdf,)\n",
    "\n",
    "    # [2~3], [2~5] 의 2개 구간에 대해서 최소값/최대값을 구한다\n",
    "    for begin, end in [(2,3),(2,5)]:\n",
    "        prods = [\"%s_prev%s\" % (prod, i) for i in range(begin,end+1)]\n",
    "        mp_df = train_df.as_matrix(columns=prods)\n",
    "\n",
    "        minf = \"%s_min_%s_%s\"%(prod,begin,end)\n",
    "        train_df[minf] = np.nanmin(mp_df, axis=1).astype(np.int8)\n",
    "\n",
    "        maxf = \"%s_max_%s_%s\"%(prod,begin,end)\n",
    "        train_df[maxf] = np.nanmax(mp_df, axis=1).astype(np.int8)\n",
    "\n",
    "        features += (minf,maxf,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고객 고유 식별 번호(ncodpers), 정수로 표현한 날짜(int_date), 실제 날짜(fecha_dato), 24개의 금융 변수(products)와 학습에 사용하기 위해 전처리/피쳐 엔지니어링한 변수(features)가 주요 변수이다.\n",
    "leave_columns = [\"ncodpers\", \"int_date\", \"fecha_dato\"] + list(products) + list(features)\n",
    "# 중복값이 없는지 확인한다\n",
    "assert len(leave_columns) == len(set(leave_columns))\n",
    "# train_df에서 주요 변수만을 추출한다\n",
    "train_df = train_df[leave_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(        ncodpers  int_date  fecha_dato  ind_ahor_fin_ult1  ind_aval_fin_ult1  \\\n",
       " 0          15889         6  2015-06-28                  0                  0   \n",
       " 1          15889        12  2015-12-28                  0                  0   \n",
       " 2          15889         7  2015-07-28                  0                  0   \n",
       " 3          15889         9  2015-09-28                  0                  0   \n",
       " 4          15889        14  2016-02-28                  0                  0   \n",
       " ...          ...       ...         ...                ...                ...   \n",
       " 998470    140745        18  2016-06-28                  0                  0   \n",
       " 998471    140747        18  2016-06-28                  0                  0   \n",
       " 998472    140748        18  2016-06-28                  0                  0   \n",
       " 998473    140757        18  2016-06-28                  0                  0   \n",
       " 998474    131627        18  2016-06-28                  0                  0   \n",
       " \n",
       "         ind_cco_fin_ult1  ind_cder_fin_ult1  ind_cno_fin_ult1  \\\n",
       " 0                      1                  0                 0   \n",
       " 1                      1                  0                 0   \n",
       " 2                      1                  0                 0   \n",
       " 3                      1                  0                 0   \n",
       " 4                      1                  0                 0   \n",
       " ...                  ...                ...               ...   \n",
       " 998470                 0                  0                 0   \n",
       " 998471                 0                  0                 0   \n",
       " 998472                 0                  0                 0   \n",
       " 998473                 0                  0                 0   \n",
       " 998474                 0                  0                 0   \n",
       " \n",
       "         ind_ctju_fin_ult1  ind_ctma_fin_ult1  ...  ind_nom_pens_ult1_max_2_3  \\\n",
       " 0                       0                  0  ...                          0   \n",
       " 1                       0                  0  ...                          0   \n",
       " 2                       0                  0  ...                          0   \n",
       " 3                       0                  0  ...                          0   \n",
       " 4                       0                  0  ...                          0   \n",
       " ...                   ...                ...  ...                        ...   \n",
       " 998470                  0                  0  ...                          0   \n",
       " 998471                  0                  0  ...                          0   \n",
       " 998472                  0                  0  ...                          0   \n",
       " 998473                  0                  0  ...                          1   \n",
       " 998474                  0                  0  ...                          0   \n",
       " \n",
       "         ind_nom_pens_ult1_min_2_5  ind_nom_pens_ult1_max_2_5  \\\n",
       " 0                               0                          0   \n",
       " 1                               0                          0   \n",
       " 2                               0                          0   \n",
       " 3                               0                          0   \n",
       " 4                               0                          0   \n",
       " ...                           ...                        ...   \n",
       " 998470                          0                          0   \n",
       " 998471                          0                          0   \n",
       " 998472                          0                          0   \n",
       " 998473                          0                          1   \n",
       " 998474                          0                          0   \n",
       " \n",
       "         ind_recibo_ult1_std_1_3  ind_recibo_ult1_std_1_5  \\\n",
       " 0                           0.0                      0.0   \n",
       " 1                           0.0                      0.0   \n",
       " 2                           0.0                      0.0   \n",
       " 3                           0.0                      0.0   \n",
       " 4                           0.0                      0.0   \n",
       " ...                         ...                      ...   \n",
       " 998470                      0.0                      0.0   \n",
       " 998471                      0.0                      0.0   \n",
       " 998472                      0.0                      0.0   \n",
       " 998473                      0.0                      0.0   \n",
       " 998474                      0.0                      0.0   \n",
       " \n",
       "         ind_recibo_ult1_std_2_5  ind_recibo_ult1_min_2_3  \\\n",
       " 0                           0.0                        0   \n",
       " 1                           0.0                        0   \n",
       " 2                           0.0                        0   \n",
       " 3                           0.0                        0   \n",
       " 4                           0.0                        0   \n",
       " ...                         ...                      ...   \n",
       " 998470                      0.0                        0   \n",
       " 998471                      0.0                        0   \n",
       " 998472                      0.0                        0   \n",
       " 998473                      0.0                        1   \n",
       " 998474                      0.0                        0   \n",
       " \n",
       "         ind_recibo_ult1_max_2_3  ind_recibo_ult1_min_2_5  \\\n",
       " 0                             0                        0   \n",
       " 1                             0                        0   \n",
       " 2                             0                        0   \n",
       " 3                             0                        0   \n",
       " 4                             0                        0   \n",
       " ...                         ...                      ...   \n",
       " 998470                        0                        0   \n",
       " 998471                        0                        0   \n",
       " 998472                        0                        0   \n",
       " 998473                        1                        1   \n",
       " 998474                        0                        0   \n",
       " \n",
       "         ind_recibo_ult1_max_2_5  \n",
       " 0                             0  \n",
       " 1                             0  \n",
       " 2                             0  \n",
       " 3                             0  \n",
       " 4                             0  \n",
       " ...                         ...  \n",
       " 998470                        0  \n",
       " 998471                        0  \n",
       " 998472                        0  \n",
       " 998473                        1  \n",
       " 998474                        0  \n",
       " \n",
       " [998475 rows x 278 columns],\n",
       " ('canal_entrada',\n",
       "  'pais_residencia',\n",
       "  'age',\n",
       "  'renta',\n",
       "  'renta_top',\n",
       "  'antiguedad',\n",
       "  'tipodom',\n",
       "  'cod_prov',\n",
       "  'fecha_dato_month',\n",
       "  'fecha_dato_year',\n",
       "  'fecha_alta_month',\n",
       "  'fecha_alta_year',\n",
       "  'dato_minus_alta',\n",
       "  'indresi_n',\n",
       "  'indext_s',\n",
       "  'conyuemp_n',\n",
       "  'sexo_h',\n",
       "  'sexo_v',\n",
       "  'ind_empleado_a',\n",
       "  'ind_empleado_b',\n",
       "  'ind_empleado_f',\n",
       "  'ind_empleado_n',\n",
       "  'ind_nuevo_new',\n",
       "  'segmento_top',\n",
       "  'segmento_particulares',\n",
       "  'segmento_universitario',\n",
       "  'indfall_s',\n",
       "  'tiprel_1mes_a',\n",
       "  'tiprel_1mes_i',\n",
       "  'tiprel_1mes_p',\n",
       "  'tiprel_1mes_r',\n",
       "  'indrel_1',\n",
       "  'indrel_99',\n",
       "  'ind_actividad_cliente',\n",
       "  'indrel_1mes',\n",
       "  'ind_ahor_fin_ult1_prev1',\n",
       "  'ind_aval_fin_ult1_prev1',\n",
       "  'ind_cco_fin_ult1_prev1',\n",
       "  'ind_cder_fin_ult1_prev1',\n",
       "  'ind_cno_fin_ult1_prev1',\n",
       "  'ind_ctju_fin_ult1_prev1',\n",
       "  'ind_ctma_fin_ult1_prev1',\n",
       "  'ind_ctop_fin_ult1_prev1',\n",
       "  'ind_ctpp_fin_ult1_prev1',\n",
       "  'ind_deco_fin_ult1_prev1',\n",
       "  'ind_deme_fin_ult1_prev1',\n",
       "  'ind_dela_fin_ult1_prev1',\n",
       "  'ind_ecue_fin_ult1_prev1',\n",
       "  'ind_fond_fin_ult1_prev1',\n",
       "  'ind_hip_fin_ult1_prev1',\n",
       "  'ind_plan_fin_ult1_prev1',\n",
       "  'ind_pres_fin_ult1_prev1',\n",
       "  'ind_reca_fin_ult1_prev1',\n",
       "  'ind_tjcr_fin_ult1_prev1',\n",
       "  'ind_valo_fin_ult1_prev1',\n",
       "  'ind_viv_fin_ult1_prev1',\n",
       "  'ind_nomina_ult1_prev1',\n",
       "  'ind_nom_pens_ult1_prev1',\n",
       "  'ind_recibo_ult1_prev1',\n",
       "  'ind_ahor_fin_ult1_prev2',\n",
       "  'ind_aval_fin_ult1_prev2',\n",
       "  'ind_cco_fin_ult1_prev2',\n",
       "  'ind_cder_fin_ult1_prev2',\n",
       "  'ind_cno_fin_ult1_prev2',\n",
       "  'ind_ctju_fin_ult1_prev2',\n",
       "  'ind_ctma_fin_ult1_prev2',\n",
       "  'ind_ctop_fin_ult1_prev2',\n",
       "  'ind_ctpp_fin_ult1_prev2',\n",
       "  'ind_deco_fin_ult1_prev2',\n",
       "  'ind_deme_fin_ult1_prev2',\n",
       "  'ind_dela_fin_ult1_prev2',\n",
       "  'ind_ecue_fin_ult1_prev2',\n",
       "  'ind_fond_fin_ult1_prev2',\n",
       "  'ind_hip_fin_ult1_prev2',\n",
       "  'ind_plan_fin_ult1_prev2',\n",
       "  'ind_pres_fin_ult1_prev2',\n",
       "  'ind_reca_fin_ult1_prev2',\n",
       "  'ind_tjcr_fin_ult1_prev2',\n",
       "  'ind_valo_fin_ult1_prev2',\n",
       "  'ind_viv_fin_ult1_prev2',\n",
       "  'ind_nomina_ult1_prev2',\n",
       "  'ind_nom_pens_ult1_prev2',\n",
       "  'ind_recibo_ult1_prev2',\n",
       "  'ind_ahor_fin_ult1_std_1_3',\n",
       "  'ind_ahor_fin_ult1_std_1_5',\n",
       "  'ind_ahor_fin_ult1_std_2_5',\n",
       "  'ind_ahor_fin_ult1_min_2_3',\n",
       "  'ind_ahor_fin_ult1_max_2_3',\n",
       "  'ind_ahor_fin_ult1_min_2_5',\n",
       "  'ind_ahor_fin_ult1_max_2_5',\n",
       "  'ind_aval_fin_ult1_std_1_3',\n",
       "  'ind_aval_fin_ult1_std_1_5',\n",
       "  'ind_aval_fin_ult1_std_2_5',\n",
       "  'ind_aval_fin_ult1_min_2_3',\n",
       "  'ind_aval_fin_ult1_max_2_3',\n",
       "  'ind_aval_fin_ult1_min_2_5',\n",
       "  'ind_aval_fin_ult1_max_2_5',\n",
       "  'ind_cco_fin_ult1_std_1_3',\n",
       "  'ind_cco_fin_ult1_std_1_5',\n",
       "  'ind_cco_fin_ult1_std_2_5',\n",
       "  'ind_cco_fin_ult1_min_2_3',\n",
       "  'ind_cco_fin_ult1_max_2_3',\n",
       "  'ind_cco_fin_ult1_min_2_5',\n",
       "  'ind_cco_fin_ult1_max_2_5',\n",
       "  'ind_cder_fin_ult1_std_1_3',\n",
       "  'ind_cder_fin_ult1_std_1_5',\n",
       "  'ind_cder_fin_ult1_std_2_5',\n",
       "  'ind_cder_fin_ult1_min_2_3',\n",
       "  'ind_cder_fin_ult1_max_2_3',\n",
       "  'ind_cder_fin_ult1_min_2_5',\n",
       "  'ind_cder_fin_ult1_max_2_5',\n",
       "  'ind_cno_fin_ult1_std_1_3',\n",
       "  'ind_cno_fin_ult1_std_1_5',\n",
       "  'ind_cno_fin_ult1_std_2_5',\n",
       "  'ind_cno_fin_ult1_min_2_3',\n",
       "  'ind_cno_fin_ult1_max_2_3',\n",
       "  'ind_cno_fin_ult1_min_2_5',\n",
       "  'ind_cno_fin_ult1_max_2_5',\n",
       "  'ind_ctju_fin_ult1_std_1_3',\n",
       "  'ind_ctju_fin_ult1_std_1_5',\n",
       "  'ind_ctju_fin_ult1_std_2_5',\n",
       "  'ind_ctju_fin_ult1_min_2_3',\n",
       "  'ind_ctju_fin_ult1_max_2_3',\n",
       "  'ind_ctju_fin_ult1_min_2_5',\n",
       "  'ind_ctju_fin_ult1_max_2_5',\n",
       "  'ind_ctma_fin_ult1_std_1_3',\n",
       "  'ind_ctma_fin_ult1_std_1_5',\n",
       "  'ind_ctma_fin_ult1_std_2_5',\n",
       "  'ind_ctma_fin_ult1_min_2_3',\n",
       "  'ind_ctma_fin_ult1_max_2_3',\n",
       "  'ind_ctma_fin_ult1_min_2_5',\n",
       "  'ind_ctma_fin_ult1_max_2_5',\n",
       "  'ind_ctop_fin_ult1_std_1_3',\n",
       "  'ind_ctop_fin_ult1_std_1_5',\n",
       "  'ind_ctop_fin_ult1_std_2_5',\n",
       "  'ind_ctop_fin_ult1_min_2_3',\n",
       "  'ind_ctop_fin_ult1_max_2_3',\n",
       "  'ind_ctop_fin_ult1_min_2_5',\n",
       "  'ind_ctop_fin_ult1_max_2_5',\n",
       "  'ind_ctpp_fin_ult1_std_1_3',\n",
       "  'ind_ctpp_fin_ult1_std_1_5',\n",
       "  'ind_ctpp_fin_ult1_std_2_5',\n",
       "  'ind_ctpp_fin_ult1_min_2_3',\n",
       "  'ind_ctpp_fin_ult1_max_2_3',\n",
       "  'ind_ctpp_fin_ult1_min_2_5',\n",
       "  'ind_ctpp_fin_ult1_max_2_5',\n",
       "  'ind_deco_fin_ult1_std_1_3',\n",
       "  'ind_deco_fin_ult1_std_1_5',\n",
       "  'ind_deco_fin_ult1_std_2_5',\n",
       "  'ind_deco_fin_ult1_min_2_3',\n",
       "  'ind_deco_fin_ult1_max_2_3',\n",
       "  'ind_deco_fin_ult1_min_2_5',\n",
       "  'ind_deco_fin_ult1_max_2_5',\n",
       "  'ind_deme_fin_ult1_std_1_3',\n",
       "  'ind_deme_fin_ult1_std_1_5',\n",
       "  'ind_deme_fin_ult1_std_2_5',\n",
       "  'ind_deme_fin_ult1_min_2_3',\n",
       "  'ind_deme_fin_ult1_max_2_3',\n",
       "  'ind_deme_fin_ult1_min_2_5',\n",
       "  'ind_deme_fin_ult1_max_2_5',\n",
       "  'ind_dela_fin_ult1_std_1_3',\n",
       "  'ind_dela_fin_ult1_std_1_5',\n",
       "  'ind_dela_fin_ult1_std_2_5',\n",
       "  'ind_dela_fin_ult1_min_2_3',\n",
       "  'ind_dela_fin_ult1_max_2_3',\n",
       "  'ind_dela_fin_ult1_min_2_5',\n",
       "  'ind_dela_fin_ult1_max_2_5',\n",
       "  'ind_ecue_fin_ult1_std_1_3',\n",
       "  'ind_ecue_fin_ult1_std_1_5',\n",
       "  'ind_ecue_fin_ult1_std_2_5',\n",
       "  'ind_ecue_fin_ult1_min_2_3',\n",
       "  'ind_ecue_fin_ult1_max_2_3',\n",
       "  'ind_ecue_fin_ult1_min_2_5',\n",
       "  'ind_ecue_fin_ult1_max_2_5',\n",
       "  'ind_fond_fin_ult1_std_1_3',\n",
       "  'ind_fond_fin_ult1_std_1_5',\n",
       "  'ind_fond_fin_ult1_std_2_5',\n",
       "  'ind_fond_fin_ult1_min_2_3',\n",
       "  'ind_fond_fin_ult1_max_2_3',\n",
       "  'ind_fond_fin_ult1_min_2_5',\n",
       "  'ind_fond_fin_ult1_max_2_5',\n",
       "  'ind_hip_fin_ult1_std_1_3',\n",
       "  'ind_hip_fin_ult1_std_1_5',\n",
       "  'ind_hip_fin_ult1_std_2_5',\n",
       "  'ind_hip_fin_ult1_min_2_3',\n",
       "  'ind_hip_fin_ult1_max_2_3',\n",
       "  'ind_hip_fin_ult1_min_2_5',\n",
       "  'ind_hip_fin_ult1_max_2_5',\n",
       "  'ind_plan_fin_ult1_std_1_3',\n",
       "  'ind_plan_fin_ult1_std_1_5',\n",
       "  'ind_plan_fin_ult1_std_2_5',\n",
       "  'ind_plan_fin_ult1_min_2_3',\n",
       "  'ind_plan_fin_ult1_max_2_3',\n",
       "  'ind_plan_fin_ult1_min_2_5',\n",
       "  'ind_plan_fin_ult1_max_2_5',\n",
       "  'ind_pres_fin_ult1_std_1_3',\n",
       "  'ind_pres_fin_ult1_std_1_5',\n",
       "  'ind_pres_fin_ult1_std_2_5',\n",
       "  'ind_pres_fin_ult1_min_2_3',\n",
       "  'ind_pres_fin_ult1_max_2_3',\n",
       "  'ind_pres_fin_ult1_min_2_5',\n",
       "  'ind_pres_fin_ult1_max_2_5',\n",
       "  'ind_reca_fin_ult1_std_1_3',\n",
       "  'ind_reca_fin_ult1_std_1_5',\n",
       "  'ind_reca_fin_ult1_std_2_5',\n",
       "  'ind_reca_fin_ult1_min_2_3',\n",
       "  'ind_reca_fin_ult1_max_2_3',\n",
       "  'ind_reca_fin_ult1_min_2_5',\n",
       "  'ind_reca_fin_ult1_max_2_5',\n",
       "  'ind_tjcr_fin_ult1_std_1_3',\n",
       "  'ind_tjcr_fin_ult1_std_1_5',\n",
       "  'ind_tjcr_fin_ult1_std_2_5',\n",
       "  'ind_tjcr_fin_ult1_min_2_3',\n",
       "  'ind_tjcr_fin_ult1_max_2_3',\n",
       "  'ind_tjcr_fin_ult1_min_2_5',\n",
       "  'ind_tjcr_fin_ult1_max_2_5',\n",
       "  'ind_valo_fin_ult1_std_1_3',\n",
       "  'ind_valo_fin_ult1_std_1_5',\n",
       "  'ind_valo_fin_ult1_std_2_5',\n",
       "  'ind_valo_fin_ult1_min_2_3',\n",
       "  'ind_valo_fin_ult1_max_2_3',\n",
       "  'ind_valo_fin_ult1_min_2_5',\n",
       "  'ind_valo_fin_ult1_max_2_5',\n",
       "  'ind_viv_fin_ult1_std_1_3',\n",
       "  'ind_viv_fin_ult1_std_1_5',\n",
       "  'ind_viv_fin_ult1_std_2_5',\n",
       "  'ind_viv_fin_ult1_min_2_3',\n",
       "  'ind_viv_fin_ult1_max_2_3',\n",
       "  'ind_viv_fin_ult1_min_2_5',\n",
       "  'ind_viv_fin_ult1_max_2_5',\n",
       "  'ind_nomina_ult1_std_1_3',\n",
       "  'ind_nomina_ult1_std_1_5',\n",
       "  'ind_nomina_ult1_std_2_5',\n",
       "  'ind_nomina_ult1_min_2_3',\n",
       "  'ind_nomina_ult1_max_2_3',\n",
       "  'ind_nomina_ult1_min_2_5',\n",
       "  'ind_nomina_ult1_max_2_5',\n",
       "  'ind_nom_pens_ult1_std_1_3',\n",
       "  'ind_nom_pens_ult1_std_1_5',\n",
       "  'ind_nom_pens_ult1_std_2_5',\n",
       "  'ind_nom_pens_ult1_min_2_3',\n",
       "  'ind_nom_pens_ult1_max_2_3',\n",
       "  'ind_nom_pens_ult1_min_2_5',\n",
       "  'ind_nom_pens_ult1_max_2_5',\n",
       "  'ind_recibo_ult1_std_1_3',\n",
       "  'ind_recibo_ult1_std_1_5',\n",
       "  'ind_recibo_ult1_std_2_5',\n",
       "  'ind_recibo_ult1_min_2_3',\n",
       "  'ind_recibo_ult1_max_2_3',\n",
       "  'ind_recibo_ult1_min_2_5',\n",
       "  'ind_recibo_ult1_max_2_5'),\n",
       " ('ind_ahor_fin_ult1_prev1',\n",
       "  'ind_aval_fin_ult1_prev1',\n",
       "  'ind_cco_fin_ult1_prev1',\n",
       "  'ind_cder_fin_ult1_prev1',\n",
       "  'ind_cno_fin_ult1_prev1',\n",
       "  'ind_ctju_fin_ult1_prev1',\n",
       "  'ind_ctma_fin_ult1_prev1',\n",
       "  'ind_ctop_fin_ult1_prev1',\n",
       "  'ind_ctpp_fin_ult1_prev1',\n",
       "  'ind_deco_fin_ult1_prev1',\n",
       "  'ind_deme_fin_ult1_prev1',\n",
       "  'ind_dela_fin_ult1_prev1',\n",
       "  'ind_ecue_fin_ult1_prev1',\n",
       "  'ind_fond_fin_ult1_prev1',\n",
       "  'ind_hip_fin_ult1_prev1',\n",
       "  'ind_plan_fin_ult1_prev1',\n",
       "  'ind_pres_fin_ult1_prev1',\n",
       "  'ind_reca_fin_ult1_prev1',\n",
       "  'ind_tjcr_fin_ult1_prev1',\n",
       "  'ind_valo_fin_ult1_prev1',\n",
       "  'ind_viv_fin_ult1_prev1',\n",
       "  'ind_nomina_ult1_prev1',\n",
       "  'ind_nom_pens_ult1_prev1',\n",
       "  'ind_recibo_ult1_prev1'))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, features, prod_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 머신러닝 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-ec4f11471b23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprod_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"2016-05-28\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprod_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"2016-06-28\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_df' is not defined"
     ]
    }
   ],
   "source": [
    "train_predict(all_df, features, prod_features, \"2016-05-28\", cv=True)\n",
    "train_predict(all_df, features, prod_features, \"2016-06-28\", cv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict(all_df, features, prod_features, str_date, cv):\n",
    "    # all_df : 통합 데이터\n",
    "    # features : 학습에 사용할 변수\n",
    "    # prod_features : 24개 금융 변수\n",
    "    # str_date : 예측 결과물을 산출하는 날짜. 2016-05-28일 경우, 훈련 데이터의 일부이며 정답을 알고 있기에 교차 검증을 의미하고, 2016-06-28일 경우, 캐글에 업로드하기 위한 테스트 데이터 예측 결과물을 생성한다\n",
    "    # cv : 교차 검증 실행 여부\n",
    "\n",
    "    # str_date로 예측 결과물을 산출하는 날짜를 지정한다\n",
    "    test_date = date_to_int(str_date)\n",
    "    # 훈련 데이터는 test_date 이전의 모든 데이터를 사용한다\n",
    "    train_df = all_df[all_df.int_date < test_date]\n",
    "    # 테스트 데이터를 통합 데이터에서 분리한다\n",
    "    test_df = pd.DataFrame(all_df[all_df.int_date == test_date])\n",
    "\n",
    "    # 신규 구매 고객만을 훈련 데이터로 추출한다\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i, prod in enumerate(products):\n",
    "        prev = prod + \"_prev1\"\n",
    "        # 신규 구매 고객을 prX에 저장한다\n",
    "        prX = train_df[(train_df[prod] == 1) & (train_df[prev] == 0)]\n",
    "        # prY에는 신규 구매에 대한 label 값을 저장한다\n",
    "        prY = np.zeros(prX.shape[0], dtype=np.int8) + i\n",
    "        X.append(prX)\n",
    "        Y.append(prY)\n",
    "\n",
    "    XY = pd.concat(X)\n",
    "    Y = np.hstack(Y)\n",
    "    # XY는 신규 구매 데이터만 포함한다\n",
    "    XY[\"y\"] = Y\n",
    "\n",
    "    # 메모리에서 변수 삭제\n",
    "    del train_df\n",
    "    del all_df\n",
    "\n",
    "    # 데이터별 가중치를 계산하기 위해서 새로운 변수 (ncodpers + fecha_dato)를 생성한다\n",
    "    XY[\"ncodepers_fecha_dato\"] = XY[\"ncodpers\"].astype(str) + XY[\"fecha_dato\"]\n",
    "    uniqs, counts = np.unique(XY[\"ncodepers_fecha_dato\"], return_counts=True)\n",
    "    # 자연 상수(e)를 통해서, count가 높은 데이터에 낮은 가중치를 준다\n",
    "    weights = np.exp(1/counts - 1)\n",
    "\n",
    "    # 가중치를 XY 데이터에 추가한다\n",
    "    wdf = pd.DataFrame()\n",
    "    wdf[\"ncodepers_fecha_dato\"] = uniqs\n",
    "    wdf[\"counts\"] = counts\n",
    "    wdf[\"weight\"] = weights\n",
    "    XY = XY.merge(wdf, on=\"ncodepers_fecha_dato\")\n",
    "\n",
    "    # 교차 검증을 위하여 XY를 훈련:검증 (8:2)로 분리한다\n",
    "    mask = np.random.rand(len(XY)) < 0.8\n",
    "    XY_train = XY[mask]\n",
    "    XY_validate = XY[~mask]\n",
    "\n",
    "    # 테스트 데이터에서 가중치는 모두 1이다\n",
    "    test_df[\"weight\"] = np.ones(len(test_df), dtype=np.int8)\n",
    "\n",
    "    # 테스트 데이터에서 “신규 구매” 정답값을 추출한다. \n",
    "    test_df[\"y\"] = test_df[\"ncodpers\"]\n",
    "    Y_prev = test_df.as_matrix(columns=prod_features)\n",
    "    for prod in products:\n",
    "        prev = prod + \"_prev1\"\n",
    "        padd = prod + \"_add\"\n",
    "        # 신규 구매 여부를 구한다\n",
    "        test_df[padd] = test_df[prod] - test_df[prev]\n",
    "\n",
    "    test_add_mat = test_df.as_matrix(columns=[prod + \"_add\" for prod in products])\n",
    "    C = test_df.as_matrix(columns=[\"ncodpers\"])\n",
    "    test_add_list = [list() for i in range(len(C))]\n",
    "    # 평가 척도 MAP@7 계산을 위하여, 고객별 신규 구매 정답값을 test_add_list에 기록한다\n",
    "    count = 0\n",
    "    for c in range(len(C)):\n",
    "        for p in range(len(products)):\n",
    "            if test_add_mat[c,p] > 0:\n",
    "                test_add_list[c].append(p)\n",
    "                count += 1\n",
    "    \n",
    "    # 교차 검증에서, 테스트 데이터로 분리된 데이터가 얻을 수 있는 최대 MAP@7 값을 계산한다. \n",
    "    if cv:\n",
    "        max_map7 = mapk(test_add_list, test_add_list, 7, 0.0)\n",
    "        map7coef = float(len(test_add_list)) / float(sum([int(bool(a)) for a in test_add_list]))\n",
    "        print(\"Max MAP@7\", str_date, max_map7, max_map7*map7coef)\n",
    "\n",
    "    # LightGBM 모델 학습 후, 예측 결과물을 저장한다\n",
    "    Y_test_lgbm = engines.lightgbm(XY_train, XY_validate, test_df, features, XY_all = XY, restore = (str_date == \"2016-06-28\"))\n",
    "    test_add_list_lightgbm = make_submission(io.BytesIO() if cv else gzip.open(\"tmp/%s.lightgbm.csv.gz\" % str_date, \"wb\"), Y_test_lgbm - Y_prev, C)\n",
    "\n",
    "    # 교차 검증일 경우, LightGBM 모델의 테스트 데이터 MAP@7 평가 척도를 출력한다\n",
    "    if cv:\n",
    "        map7lightgbm = mapk(test_add_list, test_add_list_lightgbm, 7, 0.0)\n",
    "        print(\"LightGBMlib MAP@7\", str_date, map7lightgbm, map7lightgbm*map7coef)\n",
    "\n",
    "    # XGBoost 모델 학습 후, 예측 결과물을 저장한다\n",
    "    Y_test_xgb = engines.xgboost(XY_train, XY_validate, test_df, features, XY_all = XY, restore = (str_date == \"2016-06-28\"))\n",
    "    test_add_list_xgboost = make_submission(io.BytesIO() if cv else gzip.open(\"tmp/%s.xgboost.csv.gz\" % str_date, \"wb\"), Y_test_xgb - Y_prev, C)\n",
    "\n",
    "    # 교차 검증일 경우, XGBoost 모델의 테스트 데이터 MAP@7 평가 척도를 출력한다\n",
    "    if cv:\n",
    "        map7xgboost = mapk(test_add_list, test_add_list_xgboost, 7, 0.0)\n",
    "        print(\"XGBoost MAP@7\", str_date, map7xgboost, map7xgboost*map7coef)\n",
    "\n",
    "    # 곱셈 후, 제곱근을 구하는 방식으로 앙상블을 수행한다\n",
    "    Y_test = np.sqrt(np.multiply(Y_test_xgb, Y_test_lgbm))\n",
    "    # 앙상블 결과물을 저장하고, 테스트 데이터에 대한 MAP@7 를 출력한다\n",
    "    test_add_list_xl = make_submission(io.BytesIO() if cv else gzip.open(\"tmp/%s.xgboost-lightgbm.csv.gz\" % str_date, \"wb\"), Y_test - Y_prev, C)\n",
    "\n",
    "    # 정답값인 test_add_list와 앙상블 모델의 예측값을 mapk 함수에 넣어, 평가 척도 점수를 확인한다\n",
    "    if cv:\n",
    "        map7xl = mapk(test_add_list, test_add_list_xl, 7, 0.0)\n",
    "        print(\"XGBoost+LightGBM MAP@7\", str_date, map7xl, map7xl*map7coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# xgboost, lightgbm 라이브러리\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost 모델을 학습하는 함수이다\n",
    "def xgboost(XY_train, XY_validate, test_df, features, XY_all=None, restore=False):\n",
    "    # 최적의 parameter를 지정한다\n",
    "    param = {\n",
    "        'objective': 'multi:softprob',\n",
    "        'eta': 0.1,\n",
    "        'min_child_weight': 10,\n",
    "        'max_depth': 8,\n",
    "        'silent': 1,\n",
    "        # 'nthread': 16,\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'colsample_bytree': 0.8,\n",
    "        'colsample_bylevel': 0.9,\n",
    "        'num_class': len(products),\n",
    "    }\n",
    "\n",
    "    if not restore:\n",
    "        # 훈련 데이터에서 X, Y, weight를 추출한다. as_matrix를 통해 메모리 효율적으로 array만 저장한다\n",
    "        X_train = XY_train.as_matrix(columns=features)\n",
    "        Y_train = XY_train.as_matrix(columns=[\"y\"])\n",
    "        W_train = XY_train.as_matrix(columns=[\"weight\"])\n",
    "        # xgboost 전용 데이터형식으로 변환한다\n",
    "        train = xgb.DMatrix(X_train, label=Y_train, feature_names=features, weight=W_train)\n",
    "\n",
    "        # 검증 데이터에 대해서 동일한 작업을 진행한다\n",
    "        X_validate = XY_validate.as_matrix(columns=features)\n",
    "        Y_validate = XY_validate.as_matrix(columns=[\"y\"])\n",
    "        W_validate = XY_validate.as_matrix(columns=[\"weight\"])\n",
    "        validate = xgb.DMatrix(X_validate, label=Y_validate, feature_names=features, weight=W_validate)\n",
    "\n",
    "        # XGBoost 모델을 학습한다. early_stop 조건은 20번이며, 최대 1000개의 트리를 학습한다\n",
    "        evallist  = [(train,'train'), (validate,'eval')]\n",
    "        model = xgb.train(param, train, 1000, evals=evallist, early_stopping_rounds=20)\n",
    "        # 학습된 모델을 저장한다\n",
    "        pickle.dump(model, open(\"next_multi.pickle\", \"wb\"))\n",
    "    else:\n",
    "        # “2016-06-28” 테스트 데이터를 사용할 시에는, 사전에 학습된 모델을 불러온다\n",
    "        model = pickle.load(open(\"next_multi.pickle\", \"rb\"))\n",
    "    # 교차 검증으로 최적의 트리 개수를 정한다\n",
    "    best_ntree_limit = model.best_ntree_limit\n",
    "\n",
    "    if XY_all is not None:\n",
    "        # 전체 훈련 데이터에 대해서 X, Y, weight 를 추출하고, XGBoost 전용 데이터 형태로 변환한다\n",
    "        X_all = XY_all.as_matrix(columns=features)\n",
    "        Y_all = XY_all.as_matrix(columns=[\"y\"])\n",
    "        W_all = XY_all.as_matrix(columns=[\"weight\"])\n",
    "        all_data = xgb.DMatrix(X_all, label=Y_all, feature_names=features, weight=W_all)\n",
    "\n",
    "        evallist  = [(all_data,'all_data')]\n",
    "        # 학습할 트리 개수를 전체 훈련 데이터가 늘어난 만큼 조정한다\n",
    "        best_ntree_limit = int(best_ntree_limit * (len(XY_train) + len(XY_validate)) / len(XY_train))\n",
    "        # 모델 학습!\n",
    "        model = xgb.train(param, all_data, best_ntree_limit, evals=evallist)\n",
    "\n",
    "    # 변수 중요도를 출력한다. 학습된 XGBoost 모델에서 .get_fscore()를 통해 변수 중요도를 확인할 수 있다\n",
    "    print(\"Feature importance:\")\n",
    "    for kv in sorted([(k,v) for k,v in model.get_fscore().items()], key=lambda kv: kv[1], reverse=True):\n",
    "        print(kv)\n",
    "\n",
    "    # 예측에 사용할 테스트 데이터를 XGBoost 전용 데이터로 변환한다. 이 때, weight는 모두 1이기에, 별도로 작업하지 않는다\n",
    "    X_test = test_df.as_matrix(columns=features)\n",
    "    test = xgb.DMatrix(X_test, feature_names=features)\n",
    "\n",
    "    # 학습된 모델을 기반으로, best_ntree_limit개의 트리를 기반으로 예측한다\n",
    "    return model.predict(test, ntree_limit=best_ntree_limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주피터 노트북에 이미지 삽입\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2Fr82C1%2FbtqzT495GKp%2FcJekkdIkbGNfennTj7DyQk%2Fimg.jpg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM\n",
    "- **LightGBM**은 기존의 gradient boosting 알고리즘과 달리 **leaf-wise(리프 중심) 트리 분할**을 사용한다.\n",
    "- 기존의 트리들은 트리의 깊이(tree depth)를 줄이기 위해서 level-wise(균형 트리) 분할을 사용한다. 다만 균형을 잡아주기 위한 연산이 추가되는 것이 단점이다. \n",
    "- lightgbm은 트리의 균형은 맞추지 않고 리프 노드를 지속적으로 분할하면서 진행한다. 따라서 비대칭적이고 깊은 트리가 생성되지만 동일한 leaf을 생성할 때 leaf-wise는 **level-wise보다 손실을 줄일 수 있다는 장점이 있다. 단, 데이터의 크기가 작은 경우 leaf-wise는 과적합(overfitting)되기 쉬우므로 max_depth를 줄여줘야 한다. 경험적으로 데이터의 개수(행 수)가 10,000개 이상일 때 추천한다. **\n",
    "\n",
    "### LightGBM의 주요 하이퍼 파라미터\n",
    "- objective: regression. binary, multiclass 중 선택 \n",
    "- metric: mae, rmse, mape, binary_logloss, auc, cross_entropy, .. 중 선택\n",
    "\n",
    "- learning_rate: 학습률(훈련량). 일반적으로 0.01 ~ 0.1 정도로 맞추고 다른 파라미터를 튜닝함. 나중에 성능을 더 높일 때 learning rate를 더 줄인다. \n",
    "- num_iterations: 반복하려는 트리의 개수. 기본값이 100인데 1000 정도는 해주는 게 좋다. 너무 크게 하면 과적합 발생할 수 있다. early_stopping이 있으면 최대한 많이 줘도(10,000~) 별 상관이 없다. 같은 뜻으로 사용되는 옵션(num_iteration, n_iter, num_tree, num_tress, num_round, num_rounds, num_boost_round, n_estimators) \n",
    "- max_depth: 트리의 최대 깊이. -1로 설정하면 제한 없이 분기한다. feature가 많다면 크게 설정한다. 파라미터 설정시 우선적으로 설정한다. \n",
    "- boosting_type: gbdt, rf, dart, goss 중 선택. 기본값은 gbdt이며 정확도가 중요할 때는 딥러닝의 드랍아웃과 같은 dart를 사용한다. 샘플링을 이용하는 goss도 있다. \n",
    "- bagging_fraction: 배깅을 하기 위해서 데이터를 랜덤 샘플링하여 학습에 사용한다. 비율은 0 < fraction <= 1이며 0이 되지 않게 해야한다. \n",
    "- feature_fraction: 열 샘플링(트리를 학습할 때마다 선택하는 feature의 비율). 1보다 작다면 LGBM은 매 iteration(tree)마다 다른 feature를 랜덤하게 추출하여 학습하게 된다. 만약 0.8로 값을 설정하면 매 tree를 구성할 때, feature의 80%만 랜덤하게 선택한다. 과적합을 방지하기 위해 사용할 수 있으며 학습속도가 향상된다. \n",
    "- scale_pos_weight: 클래스 불균형의 데이터 셋에서 weight를 주는 방식으로 positive를 증가시킨다. 기본값은 1이며 불균형의 정도에 따라 조절한다. \n",
    "- max_bin: 적게 주면 빠르게 계산하고 많이 주면 느려지지만 조금더 이상적인 트리 분기를 찾는다. 기본값은 255로 그냥 놔두는 편.\n",
    "- early_stopping_round: validation 셋에서 평가지표가 더이상 향상되지 않으면 학습을 정지한다. 평가지표의 향상이 n round 이상 지속되면 학습을 정지한다. \n",
    "- reg_lambda: L2 규제 \n",
    "- reg_alpha: L1 규제 \n",
    "\n",
    "#### 더 빠른 속도\n",
    "- bagging_fraction\n",
    "- max_bin은 작게\n",
    "- save_binary를 쓰면 데이터 로딩 속도가 빨라짐\n",
    "- parallel learning 사용\n",
    "\n",
    "#### 더 높은 정확도 \n",
    "- max_bin은 크게 \n",
    "- num_iterations는 크게, learning_rate은 작게 \n",
    "- num_leaves를 크게(과적합의 원인이 될 수 있음) \n",
    "- boosting 알고리즘 'dart' 사용 \n",
    "\n",
    "#### 과적합 줄이기 \n",
    "- max_bin은 작게 \n",
    "- num_leaves를 작게\n",
    "- min_data_in_leaf와 min_sum_hessian_in_leaf 사용  \n",
    "\n",
    "- min_child_samples: 리프 노드가 되기 위한 최소한의 샘플 데이터 수\n",
    "- num_leaves: 하나의 트리가 가질 수 있는 최대 리프 개수\n",
    "\n",
    "\n",
    "참고: http://machinelearningkorea.com/2019/09/29/lightgbm-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightgbm(XY_train, XY_validate, test_df, features, XY_all=None, restore=False):\n",
    "    # 훈련 데이터, 검증 데이터 X, Y, weight 추출 후, LightGBM 전용 데이터로 변환한다\n",
    "    train = lgbm.Dataset(XY_train[list(features)], label=XY_train[\"y\"], weight=XY_train[\"weight\"], feature_name=features)\n",
    "    validate = lgbm.Dataset(XY_validate[list(features)], label=XY_validate[\"y\"], weight=XY_validate[\"weight\"], feature_name=features, reference=train)\n",
    "\n",
    "    # 다양한 실험을 통해 얻은 최적의 학습 parameter\n",
    "    params = {\n",
    "        'task' : 'train',\n",
    "        'boosting_type' : 'gbdt', # 부스팅 알고리즘. gbdt, rf, dart, goss 중 선택  \n",
    "        'objective' : 'multiclass',  # regression. binary, multiclass 중 선택 \n",
    "        'num_class': 24, # 클래스 개수(복수의 클래스를 가진 분류모델을 만들 때 사용)\n",
    "        'metric' : {'multi_logloss'}, # 평가 지표. mae, rmse, mape, binary_logloss, auc, cross_entropy, .. 중 선택\n",
    "        'is_training_metric': True, # 학습시키면서 평가 지표 보고 싶을 때 \n",
    "        'max_bin': 255,  # 히스토그램 빈 개수: 기본값은 255로 그냥 놔두는 편. \n",
    "        'num_leaves' : 64,  # 하나의 트리가 가질 수 있는 최대 리프 개수\n",
    "        'learning_rate' : 0.1,  # 학습률: 일반적으로 0.01 ~ 0.1 정도 \n",
    "        'feature_fraction' : 0.8,  # 열 샘플링(트리를 학습할 때마다 선택하는 feature의 비율): 1이 기본값이나 일반적으로 0.7~0.9로 세팅 \n",
    "        'min_data_in_leaf': 10,  # 하나의 잎이 가지는 데이터의 최소량. 과적합 줄임  \n",
    "        'min_sum_hessian_in_leaf': 5,  # 잎 하나당 최소 hessian 행렬합. 과적합 줄임 \n",
    "        # 'num_threads': 16,\n",
    "    }\n",
    "\n",
    "    if not restore: # 만일 저장된 모델과 파라미터를 불러오지 못하면,\n",
    "        # XGBoost와 동일하게 훈련/검증 데이터를 기반으로 최적의 트리 개수를 계산한다\n",
    "        model = lgbm.train(params, train, num_boost_round=1000, valid_sets=validate, early_stopping_rounds=20)\n",
    "        best_iteration = model.best_iteration\n",
    "        # 학습된 모델과 최적의 트리 개수 정보를 저장한다\n",
    "        model.save_model(\"tmp/lgbm.model.txt\")\n",
    "        pickle.dump(best_iteration, open(\"tmp/lgbm.model.meta\", \"wb\"))  # 'wb'는 데이터 저장 \n",
    "    else:\n",
    "        model = lgbm.Booster(model_file=\"tmp/lgbm.model.txt\")\n",
    "        best_iteration = pickle.load(open(\"tmp/lgbm.model.meta\", \"rb\")) # 'rb'는 데이터 로딩 \n",
    "\n",
    "    if XY_all is not None:\n",
    "        # 전체 훈련 데이터에는 늘어난 양만큼 트리 개수를 늘린다\n",
    "        best_iteration = int(best_iteration * len(XY_all) / len(XY_train))\n",
    "        # 전체 훈련 데이터에 대한 LightGBM 전용 데이터를 생성한다\n",
    "        all_train = lgbm.Dataset(XY_all[list(features)], label=XY_all[\"y\"], weight=XY_all[\"weight\"], feature_name=features)\n",
    "        # LightGBM 모델 학습!\n",
    "        model = lgbm.train(params, all_train, num_boost_round=best_iteration)\n",
    "        model.save_model(\"tmp/lgbm.all.model.txt\")\n",
    "\n",
    "    # LightGBM 모델이 제공하는 변수 중요도 기능을 통해 변수 중요도를 출력한다\n",
    "    # gain: feature의 평균 이득(분할에 각 변수를 사용할 때마다 감소한 평균 훈련 손실), split: 데이터를 나누는 데 feature가 사용된 횟수 \n",
    "    print(\"Feature importance by split:\")\n",
    "    for kv in sorted([(k,v) for k,v in zip(features, model.feature_importance(\"split\"))], key=lambda kv: kv[1], reverse=True):\n",
    "        print(kv)\n",
    "    print(\"Feature importance by gain:\")\n",
    "    for kv in sorted([(k,v) for k,v in zip(features, model.feature_importance(\"gain\"))], key=lambda kv: kv[1], reverse=True):\n",
    "        print(kv)\n",
    "\n",
    "    # 테스트 데이터에 대한 예측 결과물을 return한다\n",
    "    return model.predict(test_df[list(features)], num_iteration=best_iteration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightgbm(), xgboost()로 얻은 예측 결과물을 캐글 제출용 파일로 저장한다. \n",
    "def make_submission(f, Y_test, C):\n",
    "    Y_ret = []\n",
    "    # 파일의 첫 줄에 header를 쓴다\n",
    "    f.write(\"ncodpers,added_products\\n\".encode('utf-8'))\n",
    "    # 고객 식별 번호(C)와, 예측 결과물(Y_test)의 for loop\n",
    "    for c, y_test in zip(C, Y_test):\n",
    "        # (확률값, 금융 변수명, 금융 변수 id)의 tuple을 구한다\n",
    "        y_prods = [(y,p,ip) for y,p,ip in zip(y_test, products, range(len(products)))]\n",
    "        # 확률값을 기준으로 상위 7개 결과만 추출한다\n",
    "        y_prods = sorted(y_prods, key=lambda a: a[0], reverse=True)[:7]\n",
    "        # 금융 변수 id를 Y_ret에 저장한다\n",
    "        Y_ret.append([ip for y,p,ip in y_prods])\n",
    "        y_prods = [p for y,p,ip in y_prods]\n",
    "        # 파일에 “고객 식별 번호, 7개의 금융 변수”를 쓴다\n",
    "        f.write((\"%s,%s\\n\" % (int(c), \" \".join(y_prods))).encode('utf-8'))  # c(고객식별번호), \" \"(금융변수 상위 7개)\n",
    "    # 상위 7개 예측값을 반환한다\n",
    "    return Y_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>START</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ESAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>20200330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      START\n",
       "0    Python\n",
       "1      ESAA\n",
       "2  20200330"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [참고] 텍스트 파일 쓰기 \n",
    "L=[\"Python\", \"ESAA\", \"20200330\"]\n",
    "file=open('../input/textfile.txt','w')  # 결과는 textfile.txt 형식으로 input 폴더에 저장됩니다.\n",
    "\n",
    "file.write(\"START\\n\")\n",
    "\n",
    "for i in range(3):\n",
    "    file.write('%s\\n' %L[i])\n",
    "    \n",
    "## 결과 확인\n",
    "test = pd.read_csv('../input/textfile.txt')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-338116ba428c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_add_list_lightgbm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_submission\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mgzip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tmp/%s.lightgbm.csv.gz\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mstr_date\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test_lgbm\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mY_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'cv' is not defined"
     ]
    }
   ],
   "source": [
    "test_add_list_lightgbm = make_submission(io.BytesIO() if cv else gzip.open(\"tmp/%s.lightgbm.csv.gz\"%str_date, \"wb\"), Y_test_lgbm - Y_prev, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "io.BytesIO: 메모리에 있는 바이트 배열을 파일처럼 다룰 수 있게 해주는 클래스. 유사한 클래스로 io.StringIO(문자열을 텍스트 파일처럼 취급할 수 있게 해줌)가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 캐글 업로드\n",
    "\n",
    "캐글에 업로드하면 xgboost < lightgbm < xgboost+lightgbm 순으로 성능이 좋다는 것을 알 수 있다. \n",
    "소수점 이하 5자리 수준의 차이이지만 캐글 경진대회에서는 0.1%으로도 결과의 판가름이 난다고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 요약"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 코드 스크립트 구성: main.py에서 파이프라인 모두 수행, engines.py에 모델 학습 관련 주요 함수, utils.py에 반복해서 사용하는 도구 함수 \n",
    "- 데이터 전처리: 훈련+테스트 하나로 통합, 결측값은 대부분 0.0으로 대체\n",
    "- 피쳐 엔지니어링: 다양한 방법을 사용. 범주형은 LabelEncoder, OneHotEncoder 함수로 수치형으로 변환, 빈도수 상위 100개의 순위 생성, 'renta'의 log 정규화, 날짜 변수간 차이값 사용, lag-5의 변수 구간별 기초통계량 사용, 고객 간 빈도수 조절 \n",
    "- 하이퍼 파라미터: 2016-05-28 데이터를 테스트 데이터로 사용하여 MAP@7 점수를 측정, 훈련 데이터를 8:2로 나누어 교차 검증 수행 \n",
    "- 모델: XGBoost와 LightGBM 모델 사용 \n",
    "\n",
    "#### 피처 엔지니어링의 차이가 1,077등과 15등의 차이를 만들었다. Baseline 모델과 8등 팀의 XGBoost 모델의 파라미터는 동일하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
