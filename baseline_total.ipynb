{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline 모델 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. 패키지 설치 / 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed 고정 \n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chanmi Yoo\\Desktop\\ESAA\\Machine Learning with Kaggle Winners\\code1\\kaggle_santander_product_recommendation\\02_Baseline\\code\n"
     ]
    }
   ],
   "source": [
    "# 현재 디렉토리 확인 \n",
    "import os\n",
    "print (os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chanmi Yoo\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (5,8,11,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 불러온다. \n",
    "trn = pd.read_csv('../../data/train_ver2.csv') # 다운 받은 파일이 해당 경로에 있는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chanmi Yoo\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "tst = pd.read_csv('../../data/test_ver2.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 데이터 전처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제품 변수를 별도로 저장해 놓는다.\n",
    "prods = trn.columns[24:].tolist()\n",
    "\n",
    "# 제품 변수 결측값을 미리 0으로 대체한다.\n",
    "trn[prods] = trn[prods].fillna(0.0).astype(np.int8)\n",
    "\n",
    "# 24개 제품 중 하나도 보유하지 않는 고객 데이터를 제거한다.\n",
    "no_product = trn[prods].sum(axis=1) == 0  # axis=1은 y축 합계 \n",
    "trn = trn[~no_product]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터와 테스트 데이터를 통합한다. 테스트 데이터에 없는 제품 변수는 0으로 채운다.\n",
    "for col in trn.columns[24:]:\n",
    "    tst[col] = 0\n",
    "df = pd.concat([trn, tst], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 사용할 변수를 담는 list이다.\n",
    "features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범주형 변수를 .factorize() 함수를 통해 label encoding한다.\n",
    "categorical_cols = ['ind_empleado', 'pais_residencia', 'sexo', 'tiprel_1mes', 'indresi', 'indext', 'conyuemp', 'canal_entrada', 'indfall', 'tipodom', 'nomprov', 'segmento']\n",
    "for col in categorical_cols:\n",
    "    df[col], _ = df[col].factorize(na_sentinel=-99)  # na_sentinel은 missing value가 -99로 표현되도록 만든다. 이때 -99는 unique한 값으로 여겨지지 않는다. \n",
    "features += categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 -1  1  2  0]\n",
      "['b' 'a' 'c']\n"
     ]
    }
   ],
   "source": [
    "## 참고: na_sentinel\n",
    "codes, uniques = pd.factorize(['b', None, 'a', 'c', 'b'])\n",
    "print(codes)\n",
    "print(uniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치형 변수의 특이값과 결측값을 -99로 대체하고, 정수형으로 변환한다.\n",
    "df['age'].replace(' NA', -99, inplace=True)\n",
    "df['age'] = df['age'].astype(np.int8)\n",
    "\n",
    "df['antiguedad'].replace('     NA', -99, inplace=True)\n",
    "df['antiguedad'] = df['antiguedad'].astype(np.int8)\n",
    "\n",
    "df['renta'].replace('         NA', -99, inplace=True)\n",
    "df['renta'].fillna(-99, inplace=True)\n",
    "df['renta'] = df['renta'].astype(float).astype(np.int8)\n",
    "\n",
    "df['indrel_1mes'].replace('P', 5, inplace=True)  # 고객 구분이 1,2,3,4,P등급으로 되어있는데, 여기서 P등급을 5로 바꾸어 줌 \n",
    "df['indrel_1mes'].fillna(-99, inplace=True)\n",
    "df['indrel_1mes'] = df['indrel_1mes'].astype(float).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 사용할 수치형 변수를 features에 추구한다.\n",
    "features += ['age','antiguedad','renta','ind_nuevo','indrel','indrel_1mes','ind_actividad_cliente'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 피쳐 엔지니어링 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "피쳐 엔지니어링 단계에서는 모델 학습에 사용한 파생 변수를 생성한다.\n",
    "ex) 날짜/시간 정보 → 주중/주말 여부, 공휴일 여부, 아침/낮/밤, 봄/여름/가을/겨울 등 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline 모델에서는 전체 24개의 고객 변수와, 4개의 날짜 변수 기반 파생 변수, 그리고 24개의 lag-1 변수를 사용한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 날짜 변수 기반 파생 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2개의 날짜 변수에서 연도와 월 정보를 추출한다.\n",
    "# fecha_alta_month(고객이 첫 계약을 맺은 날짜), ult_fec_cli_1t_month(고객이 마지막으로 1등급이었던 날짜)\n",
    "\n",
    "# fetcha_alta 변수형이 float(실수)이면 0을 반환하고, 아닐 경우 '-'를 기준으로 나누어 2번째 값을 정수형으로 표현한 뒤, fetcha_alta_month로 저장 \n",
    "df['fecha_alta_month'] = df['fecha_alta'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[1])).astype(np.int8) \n",
    "df['fecha_alta_year'] = df['fecha_alta'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[0])).astype(np.int16)\n",
    "features += ['fecha_alta_month', 'fecha_alta_year']\n",
    "\n",
    "df['ult_fec_cli_1t_month'] = df['ult_fec_cli_1t'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[1])).astype(np.int8)\n",
    "df['ult_fec_cli_1t_year'] = df['ult_fec_cli_1t'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[0])).astype(np.int16)\n",
    "features += ['ult_fec_cli_1t_month', 'ult_fec_cli_1t_year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*참고: 데이터 효율을 위해 month와 year을 각각 int8과 int16으로 표현한 것으로 보인다. int8과 int16는 각각 2^8개, 2^16개의 정수 표현이 가능하다. 즉, 각각 -128부터 127까지, -32768부터 32767까지의 정수를 표현할 수 있다. 범위가 1부터 12까지인 월을 표현할 때는 int8로 충분히 표현이 가능하지만(int16을 사용하여 메모리를 불필요하게 많이 쓰지 않아도 됨), 연도는 네 자리 수이기 때문에 int16을 사용하여 정수로 변환한 것으로 보인다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이외에도 날짜 변수를 활용하여 다양한 파생 변수를 생성할 수 있다. 예를 들어 두 개의 날짜 변수 간의 차이값을 파생 변수를 생성한다거나, 졸업식이나 방학 등의 특별한 날짜까지의 거리를 수치형 변수로 생성할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 결측값 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그 외 변수의 결측값은 모두 -99로 대체한다.\n",
    "df.fillna(-99, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사이킷런에서 제공하는 머신러닝 모델은 결측값을 입력값으로 받지 않고 실행 에러가 발생한다. 하지만 xgboost 모델에서는 결측값도 하나의 정보로 인식하고 모델 학습에 활용한다. 그러나 저자는 결측값을 -99로 설정함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lag-1 파생 변수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시계열 데이터라는 데이터 특성을 살려 고객의 과거 데이터를 기반으로 다양한 파생 변수를 생성할 수 있다. 예를 들어, 고객의 나이가 최근 3개월 동안 변동이 있었는지(즉, 3개월 안에 생일을 맞이했는지)를 이진 변수로 생성하거나, 한 달 전에 구매한 제품에 대한 정보를 변수로 사용할 수 있고, 최근 6개월 간 평균 월급을 계산할 수도 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "산탄데르 제품 추천 경진대회에서는 N개월 전에 금융 제품을 보유하고 있었는지 여부를 나타내는 lag 변수가 좋은 파생 변수로 작용했다. Baseline 모델에서는 1개월 전 정보만을 가져다 사용하는 lag-1 변수를 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날짜를 숫자로 변환하는 함수를 생성한다. 2015-01-28은 1, 2016-06-28은 18로 변환된다\n",
    "def date_to_int(str_date):\n",
    "    Y, M, D = [int(a) for a in str_date.strip().split(\"-\")]  # '-' 기준으로 날짜를 나눈 뒤 리스트 형태로 각각 Y, M, D에 저장 \n",
    "    int_date = (int(Y) - 2015) * 12 + int(M)  # 2015년 기준 개월 수를 계산 \n",
    "    return int_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날짜를 숫자로 변환하여 int_date에 저장한다\n",
    "df['int_date'] = df['fecha_dato'].map(date_to_int).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 복사하고, int_date 날짜에 1을 더하여 lag를 생성한다. ncodpers과 int_date를 제외한 변수명에 _prev를 추가한다.\n",
    "df_lag = df.copy()\n",
    "df_lag.columns = [col + '_prev' if col not in ['ncodpers', 'int_date'] else col for col in df.columns ]\n",
    "df_lag['int_date'] += 1  # 만일 lag-2, lag-5를 만들고 싶다면, 여기에 있는 숫자를 2나 5로 바꾸면 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fecha_dato</th>\n",
       "      <th>ncodpers</th>\n",
       "      <th>ind_empleado</th>\n",
       "      <th>pais_residencia</th>\n",
       "      <th>sexo</th>\n",
       "      <th>age</th>\n",
       "      <th>fecha_alta</th>\n",
       "      <th>ind_nuevo</th>\n",
       "      <th>antiguedad</th>\n",
       "      <th>indrel</th>\n",
       "      <th>...</th>\n",
       "      <th>ind_valo_fin_ult1</th>\n",
       "      <th>ind_viv_fin_ult1</th>\n",
       "      <th>ind_nomina_ult1</th>\n",
       "      <th>ind_nom_pens_ult1</th>\n",
       "      <th>ind_recibo_ult1</th>\n",
       "      <th>fecha_alta_month</th>\n",
       "      <th>fecha_alta_year</th>\n",
       "      <th>ult_fec_cli_1t_month</th>\n",
       "      <th>ult_fec_cli_1t_year</th>\n",
       "      <th>int_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2015-01-28</td>\n",
       "      <td>1375586</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>2015-01-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-28</td>\n",
       "      <td>1050611</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>2012-08-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   fecha_dato  ncodpers  ind_empleado  pais_residencia  sexo  age  fecha_alta  \\\n",
       "0  2015-01-28   1375586             0                0     0   35  2015-01-12   \n",
       "1  2015-01-28   1050611             0                0     1   23  2012-08-10   \n",
       "\n",
       "   ind_nuevo  antiguedad  indrel  ... ind_valo_fin_ult1  ind_viv_fin_ult1  \\\n",
       "0        0.0           6     1.0  ...                 0                 0   \n",
       "1        0.0          35     1.0  ...                 0                 0   \n",
       "\n",
       "   ind_nomina_ult1  ind_nom_pens_ult1  ind_recibo_ult1  fecha_alta_month  \\\n",
       "0                0                  0                0                 1   \n",
       "1                0                  0                0                 8   \n",
       "\n",
       "   fecha_alta_year  ult_fec_cli_1t_month  ult_fec_cli_1t_year  int_date  \n",
       "0             2015                     0                    0         1  \n",
       "1             2012                     0                    0         1  \n",
       "\n",
       "[2 rows x 53 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fecha_dato_prev</th>\n",
       "      <th>ncodpers</th>\n",
       "      <th>ind_empleado_prev</th>\n",
       "      <th>pais_residencia_prev</th>\n",
       "      <th>sexo_prev</th>\n",
       "      <th>age_prev</th>\n",
       "      <th>fecha_alta_prev</th>\n",
       "      <th>ind_nuevo_prev</th>\n",
       "      <th>antiguedad_prev</th>\n",
       "      <th>indrel_prev</th>\n",
       "      <th>...</th>\n",
       "      <th>ind_valo_fin_ult1_prev</th>\n",
       "      <th>ind_viv_fin_ult1_prev</th>\n",
       "      <th>ind_nomina_ult1_prev</th>\n",
       "      <th>ind_nom_pens_ult1_prev</th>\n",
       "      <th>ind_recibo_ult1_prev</th>\n",
       "      <th>fecha_alta_month_prev</th>\n",
       "      <th>fecha_alta_year_prev</th>\n",
       "      <th>ult_fec_cli_1t_month_prev</th>\n",
       "      <th>ult_fec_cli_1t_year_prev</th>\n",
       "      <th>int_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2015-01-28</td>\n",
       "      <td>1375586</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>2015-01-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-28</td>\n",
       "      <td>1050611</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>2012-08-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  fecha_dato_prev  ncodpers  ind_empleado_prev  pais_residencia_prev  \\\n",
       "0      2015-01-28   1375586                  0                     0   \n",
       "1      2015-01-28   1050611                  0                     0   \n",
       "\n",
       "   sexo_prev  age_prev fecha_alta_prev  ind_nuevo_prev  antiguedad_prev  \\\n",
       "0          0        35      2015-01-12             0.0                6   \n",
       "1          1        23      2012-08-10             0.0               35   \n",
       "\n",
       "   indrel_prev  ... ind_valo_fin_ult1_prev  ind_viv_fin_ult1_prev  \\\n",
       "0          1.0  ...                      0                      0   \n",
       "1          1.0  ...                      0                      0   \n",
       "\n",
       "   ind_nomina_ult1_prev  ind_nom_pens_ult1_prev  ind_recibo_ult1_prev  \\\n",
       "0                     0                       0                     0   \n",
       "1                     0                       0                     0   \n",
       "\n",
       "   fecha_alta_month_prev  fecha_alta_year_prev  ult_fec_cli_1t_month_prev  \\\n",
       "0                      1                  2015                          0   \n",
       "1                      8                  2012                          0   \n",
       "\n",
       "   ult_fec_cli_1t_year_prev  int_date  \n",
       "0                         0         2  \n",
       "1                         0         2  \n",
       "\n",
       "[2 rows x 53 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lag.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를 들어 날짜가 2015년 1월 28일이었다면 fecha_dato 변수에 들어간 값은 날짜형인 2015-01-28이었을 것이고, 이를 date_to_int 함수로 변환한 값인 1이 int_date에 저장된다. 그리고 _prev 변수에는 여기에 1인 더한 값인 2가 들어간다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fecha_dato였던 변수명이 fecha_dato_prev으로 바뀌었으며, 다른 변수들도 마찬가지이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 데이터와 lag 데이터를 ncodper와 int_date 기준으로 합친다. Lag 데이터의 int_date는 1 밀려 있기 때문에, 저번 달의 제품 정보가 삽입된다.\n",
    "df_trn = df.merge(df_lag, on=['ncodpers','int_date'], how='left')\n",
    "\n",
    "# 이 명령어 실행시키는 데 시간이 오래 걸리니, 유의하세요. (저는 30분 정도 걸렸습니다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 데이터셋을 합쳐 변수가 104개인 df_trn 데이터셋이 만들어진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 효율을 위해 불필요한 변수를 메모리에서 제거한다\n",
    "del df, df_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저번 달의 제품 정보가 존재하지 않을 경우를 대비하여 0으로 대체한다.\n",
    "for prod in prods:\n",
    "    prev = prod + '_prev'\n",
    "    df_trn[prev].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn.fillna(-99, inplace=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lag-1 변수를 추가한다.\n",
    "features += [feature + '_prev' for feature in features]\n",
    "features += [prod + '_prev' for prod in prods]\n",
    "\n",
    "###\n",
    "### Baseline 모델 이후, 다양한 피쳐 엔지니어링을 여기에 추가한다.\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습을 위하여 데이터를 훈련, 테스트용으로 분리한다.\n",
    "# 학습에는 2016-01-28 ~ 2016-04-28 데이터만 사용하고, 검증에는 2016-05-28 데이터를 사용한다.\n",
    "use_dates = ['2016-01-28', '2016-02-28', '2016-03-28', '2016-04-28', '2016-05-28']\n",
    "trn = df_trn[df_trn['fecha_dato'].isin(use_dates)]\n",
    "tst = df_trn[df_trn['fecha_dato'] == '2016-06-28']\n",
    "del df_trn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터에서 신규 구매 건수만 추출한다.\n",
    "X = []\n",
    "Y = []\n",
    "for i, prod in enumerate(prods):\n",
    "    prev = prod + '_prev'\n",
    "    prX = trn[(trn[prod] == 1) & (trn[prev] == 0)]\n",
    "    prY = np.zeros(prX.shape[0], dtype=np.int8) + i\n",
    "    X.append(prX)\n",
    "    Y.append(prY)\n",
    "XY = pd.concat(X)\n",
    "Y = np.hstack(Y)\n",
    "XY['y'] = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련, 검증 데이터로 분리한다. \n",
    "vld_date = '2016-05-28'\n",
    "XY_trn = XY[XY['fecha_dato'] != vld_date]\n",
    "XY_vld = XY[XY['fecha_dato'] == vld_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost 모델 parameter를 설정한다.\n",
    "param = {\n",
    "    'booster': 'gbtree',\n",
    "    'max_depth': 8,\n",
    "    'nthread': 4,\n",
    "    'num_class': len(prods),\n",
    "    'objective': 'multi:softprob',\n",
    "    'silent': 1,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'eta': 0.1,\n",
    "    'min_child_weight': 10,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'colsample_bylevel': 0.9,\n",
    "    'seed': 2018,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chanmi Yoo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "C:\\Users\\Chanmi Yoo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\Chanmi Yoo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "C:\\Users\\Chanmi Yoo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# 훈련, 검증 데이터를 XGBoost 형태로 변환한다.\n",
    "X_trn = XY_trn.as_matrix(columns=features)\n",
    "Y_trn = XY_trn.as_matrix(columns=['y'])\n",
    "dtrn = xgb.DMatrix(X_trn, label=Y_trn, feature_names=features)\n",
    "\n",
    "X_vld = XY_vld.as_matrix(columns=features)\n",
    "Y_vld = XY_vld.as_matrix(columns=['y'])\n",
    "dvld = xgb.DMatrix(X_vld, label=Y_vld, feature_names=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.72501\teval-mlogloss:2.73551\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 20 rounds.\n",
      "[1]\ttrain-mlogloss:2.45697\teval-mlogloss:2.47072\n",
      "[2]\ttrain-mlogloss:2.27435\teval-mlogloss:2.28931\n",
      "[3]\ttrain-mlogloss:2.13564\teval-mlogloss:2.15204\n",
      "[4]\ttrain-mlogloss:2.02201\teval-mlogloss:2.03829\n",
      "[5]\ttrain-mlogloss:1.92567\teval-mlogloss:1.94298\n",
      "[6]\ttrain-mlogloss:1.84739\teval-mlogloss:1.86529\n",
      "[7]\ttrain-mlogloss:1.77948\teval-mlogloss:1.79788\n",
      "[8]\ttrain-mlogloss:1.71764\teval-mlogloss:1.73641\n",
      "[9]\ttrain-mlogloss:1.66492\teval-mlogloss:1.68415\n",
      "[10]\ttrain-mlogloss:1.61811\teval-mlogloss:1.63771\n",
      "[11]\ttrain-mlogloss:1.57482\teval-mlogloss:1.59515\n",
      "[12]\ttrain-mlogloss:1.53556\teval-mlogloss:1.55617\n",
      "[13]\ttrain-mlogloss:1.50161\teval-mlogloss:1.52287\n",
      "[14]\ttrain-mlogloss:1.47083\teval-mlogloss:1.49261\n",
      "[15]\ttrain-mlogloss:1.44126\teval-mlogloss:1.46339\n",
      "[16]\ttrain-mlogloss:1.41514\teval-mlogloss:1.43768\n",
      "[17]\ttrain-mlogloss:1.39173\teval-mlogloss:1.41475\n",
      "[18]\ttrain-mlogloss:1.37001\teval-mlogloss:1.39331\n",
      "[19]\ttrain-mlogloss:1.34997\teval-mlogloss:1.37353\n",
      "[20]\ttrain-mlogloss:1.33167\teval-mlogloss:1.35551\n",
      "[21]\ttrain-mlogloss:1.31394\teval-mlogloss:1.33805\n",
      "[22]\ttrain-mlogloss:1.29823\teval-mlogloss:1.32279\n",
      "[23]\ttrain-mlogloss:1.28369\teval-mlogloss:1.30846\n",
      "[24]\ttrain-mlogloss:1.27022\teval-mlogloss:1.29545\n",
      "[25]\ttrain-mlogloss:1.25716\teval-mlogloss:1.28255\n",
      "[26]\ttrain-mlogloss:1.24525\teval-mlogloss:1.27095\n",
      "[27]\ttrain-mlogloss:1.23416\teval-mlogloss:1.26018\n",
      "[28]\ttrain-mlogloss:1.22395\teval-mlogloss:1.25025\n",
      "[29]\ttrain-mlogloss:1.21471\teval-mlogloss:1.24138\n",
      "[30]\ttrain-mlogloss:1.20609\teval-mlogloss:1.23306\n",
      "[31]\ttrain-mlogloss:1.19737\teval-mlogloss:1.22484\n",
      "[32]\ttrain-mlogloss:1.18920\teval-mlogloss:1.21703\n",
      "[33]\ttrain-mlogloss:1.18189\teval-mlogloss:1.20995\n",
      "[34]\ttrain-mlogloss:1.17495\teval-mlogloss:1.20320\n",
      "[35]\ttrain-mlogloss:1.16824\teval-mlogloss:1.19687\n",
      "[36]\ttrain-mlogloss:1.16198\teval-mlogloss:1.19099\n",
      "[37]\ttrain-mlogloss:1.15597\teval-mlogloss:1.18528\n",
      "[38]\ttrain-mlogloss:1.15054\teval-mlogloss:1.18028\n",
      "[39]\ttrain-mlogloss:1.14531\teval-mlogloss:1.17555\n",
      "[40]\ttrain-mlogloss:1.14054\teval-mlogloss:1.17127\n",
      "[41]\ttrain-mlogloss:1.13585\teval-mlogloss:1.16689\n",
      "[42]\ttrain-mlogloss:1.13162\teval-mlogloss:1.16295\n",
      "[43]\ttrain-mlogloss:1.12739\teval-mlogloss:1.15922\n",
      "[44]\ttrain-mlogloss:1.12333\teval-mlogloss:1.15554\n",
      "[45]\ttrain-mlogloss:1.11968\teval-mlogloss:1.15215\n",
      "[46]\ttrain-mlogloss:1.11623\teval-mlogloss:1.14899\n",
      "[47]\ttrain-mlogloss:1.11267\teval-mlogloss:1.14592\n",
      "[48]\ttrain-mlogloss:1.10937\teval-mlogloss:1.14302\n",
      "[49]\ttrain-mlogloss:1.10630\teval-mlogloss:1.14031\n",
      "[50]\ttrain-mlogloss:1.10331\teval-mlogloss:1.13773\n",
      "[51]\ttrain-mlogloss:1.10048\teval-mlogloss:1.13529\n",
      "[52]\ttrain-mlogloss:1.09784\teval-mlogloss:1.13305\n",
      "[53]\ttrain-mlogloss:1.09515\teval-mlogloss:1.13082\n",
      "[54]\ttrain-mlogloss:1.09274\teval-mlogloss:1.12881\n",
      "[55]\ttrain-mlogloss:1.09037\teval-mlogloss:1.12685\n",
      "[56]\ttrain-mlogloss:1.08826\teval-mlogloss:1.12506\n",
      "[57]\ttrain-mlogloss:1.08621\teval-mlogloss:1.12331\n",
      "[58]\ttrain-mlogloss:1.08420\teval-mlogloss:1.12166\n",
      "[59]\ttrain-mlogloss:1.08216\teval-mlogloss:1.12005\n",
      "[60]\ttrain-mlogloss:1.08022\teval-mlogloss:1.11857\n",
      "[61]\ttrain-mlogloss:1.07837\teval-mlogloss:1.11715\n",
      "[62]\ttrain-mlogloss:1.07656\teval-mlogloss:1.11578\n",
      "[63]\ttrain-mlogloss:1.07481\teval-mlogloss:1.11462\n",
      "[64]\ttrain-mlogloss:1.07319\teval-mlogloss:1.11339\n",
      "[65]\ttrain-mlogloss:1.07165\teval-mlogloss:1.11223\n",
      "[66]\ttrain-mlogloss:1.07026\teval-mlogloss:1.11114\n",
      "[67]\ttrain-mlogloss:1.06886\teval-mlogloss:1.11011\n",
      "[68]\ttrain-mlogloss:1.06737\teval-mlogloss:1.10912\n",
      "[69]\ttrain-mlogloss:1.06595\teval-mlogloss:1.10814\n",
      "[70]\ttrain-mlogloss:1.06458\teval-mlogloss:1.10724\n",
      "[71]\ttrain-mlogloss:1.06341\teval-mlogloss:1.10641\n",
      "[72]\ttrain-mlogloss:1.06217\teval-mlogloss:1.10565\n",
      "[73]\ttrain-mlogloss:1.06088\teval-mlogloss:1.10488\n",
      "[74]\ttrain-mlogloss:1.05975\teval-mlogloss:1.10417\n",
      "[75]\ttrain-mlogloss:1.05866\teval-mlogloss:1.10349\n",
      "[76]\ttrain-mlogloss:1.05757\teval-mlogloss:1.10288\n",
      "[77]\ttrain-mlogloss:1.05648\teval-mlogloss:1.10221\n",
      "[78]\ttrain-mlogloss:1.05541\teval-mlogloss:1.10170\n",
      "[79]\ttrain-mlogloss:1.05442\teval-mlogloss:1.10108\n",
      "[80]\ttrain-mlogloss:1.05336\teval-mlogloss:1.10050\n",
      "[81]\ttrain-mlogloss:1.05238\teval-mlogloss:1.09994\n",
      "[82]\ttrain-mlogloss:1.05138\teval-mlogloss:1.09942\n",
      "[83]\ttrain-mlogloss:1.05041\teval-mlogloss:1.09891\n",
      "[84]\ttrain-mlogloss:1.04958\teval-mlogloss:1.09845\n",
      "[85]\ttrain-mlogloss:1.04863\teval-mlogloss:1.09802\n",
      "[86]\ttrain-mlogloss:1.04782\teval-mlogloss:1.09756\n",
      "[87]\ttrain-mlogloss:1.04670\teval-mlogloss:1.09709\n",
      "[88]\ttrain-mlogloss:1.04595\teval-mlogloss:1.09672\n",
      "[89]\ttrain-mlogloss:1.04519\teval-mlogloss:1.09637\n",
      "[90]\ttrain-mlogloss:1.04445\teval-mlogloss:1.09598\n",
      "[91]\ttrain-mlogloss:1.04362\teval-mlogloss:1.09560\n",
      "[92]\ttrain-mlogloss:1.04273\teval-mlogloss:1.09527\n",
      "[93]\ttrain-mlogloss:1.04197\teval-mlogloss:1.09494\n",
      "[94]\ttrain-mlogloss:1.04119\teval-mlogloss:1.09467\n",
      "[95]\ttrain-mlogloss:1.04054\teval-mlogloss:1.09439\n",
      "[96]\ttrain-mlogloss:1.03986\teval-mlogloss:1.09415\n",
      "[97]\ttrain-mlogloss:1.03910\teval-mlogloss:1.09381\n",
      "[98]\ttrain-mlogloss:1.03848\teval-mlogloss:1.09361\n",
      "[99]\ttrain-mlogloss:1.03772\teval-mlogloss:1.09334\n",
      "[100]\ttrain-mlogloss:1.03697\teval-mlogloss:1.09309\n",
      "[101]\ttrain-mlogloss:1.03636\teval-mlogloss:1.09282\n",
      "[102]\ttrain-mlogloss:1.03570\teval-mlogloss:1.09262\n",
      "[103]\ttrain-mlogloss:1.03503\teval-mlogloss:1.09243\n",
      "[104]\ttrain-mlogloss:1.03425\teval-mlogloss:1.09219\n",
      "[105]\ttrain-mlogloss:1.03345\teval-mlogloss:1.09198\n",
      "[106]\ttrain-mlogloss:1.03281\teval-mlogloss:1.09175\n",
      "[107]\ttrain-mlogloss:1.03224\teval-mlogloss:1.09158\n",
      "[108]\ttrain-mlogloss:1.03155\teval-mlogloss:1.09151\n",
      "[109]\ttrain-mlogloss:1.03085\teval-mlogloss:1.09138\n",
      "[110]\ttrain-mlogloss:1.03025\teval-mlogloss:1.09120\n",
      "[111]\ttrain-mlogloss:1.02966\teval-mlogloss:1.09105\n",
      "[112]\ttrain-mlogloss:1.02901\teval-mlogloss:1.09090\n",
      "[113]\ttrain-mlogloss:1.02833\teval-mlogloss:1.09076\n",
      "[114]\ttrain-mlogloss:1.02774\teval-mlogloss:1.09069\n",
      "[115]\ttrain-mlogloss:1.02714\teval-mlogloss:1.09056\n",
      "[116]\ttrain-mlogloss:1.02645\teval-mlogloss:1.09042\n",
      "[117]\ttrain-mlogloss:1.02590\teval-mlogloss:1.09030\n",
      "[118]\ttrain-mlogloss:1.02516\teval-mlogloss:1.09021\n",
      "[119]\ttrain-mlogloss:1.02443\teval-mlogloss:1.09002\n",
      "[120]\ttrain-mlogloss:1.02387\teval-mlogloss:1.08994\n",
      "[121]\ttrain-mlogloss:1.02340\teval-mlogloss:1.08979\n",
      "[122]\ttrain-mlogloss:1.02285\teval-mlogloss:1.08970\n",
      "[123]\ttrain-mlogloss:1.02205\teval-mlogloss:1.08954\n",
      "[124]\ttrain-mlogloss:1.02152\teval-mlogloss:1.08942\n",
      "[125]\ttrain-mlogloss:1.02092\teval-mlogloss:1.08933\n",
      "[126]\ttrain-mlogloss:1.02037\teval-mlogloss:1.08924\n",
      "[127]\ttrain-mlogloss:1.01979\teval-mlogloss:1.08919\n",
      "[128]\ttrain-mlogloss:1.01919\teval-mlogloss:1.08911\n",
      "[129]\ttrain-mlogloss:1.01861\teval-mlogloss:1.08906\n",
      "[130]\ttrain-mlogloss:1.01799\teval-mlogloss:1.08898\n",
      "[131]\ttrain-mlogloss:1.01738\teval-mlogloss:1.08885\n",
      "[132]\ttrain-mlogloss:1.01689\teval-mlogloss:1.08880\n",
      "[133]\ttrain-mlogloss:1.01632\teval-mlogloss:1.08872\n",
      "[134]\ttrain-mlogloss:1.01580\teval-mlogloss:1.08863\n",
      "[135]\ttrain-mlogloss:1.01530\teval-mlogloss:1.08851\n",
      "[136]\ttrain-mlogloss:1.01460\teval-mlogloss:1.08849\n",
      "[137]\ttrain-mlogloss:1.01413\teval-mlogloss:1.08842\n",
      "[138]\ttrain-mlogloss:1.01355\teval-mlogloss:1.08836\n",
      "[139]\ttrain-mlogloss:1.01299\teval-mlogloss:1.08825\n",
      "[140]\ttrain-mlogloss:1.01214\teval-mlogloss:1.08814\n",
      "[141]\ttrain-mlogloss:1.01144\teval-mlogloss:1.08807\n",
      "[142]\ttrain-mlogloss:1.01084\teval-mlogloss:1.08802\n",
      "[143]\ttrain-mlogloss:1.01026\teval-mlogloss:1.08794\n",
      "[144]\ttrain-mlogloss:1.00954\teval-mlogloss:1.08788\n",
      "[145]\ttrain-mlogloss:1.00898\teval-mlogloss:1.08783\n",
      "[146]\ttrain-mlogloss:1.00848\teval-mlogloss:1.08782\n",
      "[147]\ttrain-mlogloss:1.00775\teval-mlogloss:1.08779\n",
      "[148]\ttrain-mlogloss:1.00727\teval-mlogloss:1.08774\n",
      "[149]\ttrain-mlogloss:1.00674\teval-mlogloss:1.08765\n",
      "[150]\ttrain-mlogloss:1.00626\teval-mlogloss:1.08764\n",
      "[151]\ttrain-mlogloss:1.00560\teval-mlogloss:1.08759\n",
      "[152]\ttrain-mlogloss:1.00499\teval-mlogloss:1.08753\n",
      "[153]\ttrain-mlogloss:1.00438\teval-mlogloss:1.08749\n",
      "[154]\ttrain-mlogloss:1.00389\teval-mlogloss:1.08753\n",
      "[155]\ttrain-mlogloss:1.00326\teval-mlogloss:1.08750\n",
      "[156]\ttrain-mlogloss:1.00263\teval-mlogloss:1.08747\n",
      "[157]\ttrain-mlogloss:1.00189\teval-mlogloss:1.08741\n",
      "[158]\ttrain-mlogloss:1.00137\teval-mlogloss:1.08740\n",
      "[159]\ttrain-mlogloss:1.00068\teval-mlogloss:1.08733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[160]\ttrain-mlogloss:1.00000\teval-mlogloss:1.08731\n",
      "[161]\ttrain-mlogloss:0.99943\teval-mlogloss:1.08730\n",
      "[162]\ttrain-mlogloss:0.99884\teval-mlogloss:1.08730\n",
      "[163]\ttrain-mlogloss:0.99834\teval-mlogloss:1.08726\n",
      "[164]\ttrain-mlogloss:0.99772\teval-mlogloss:1.08724\n",
      "[165]\ttrain-mlogloss:0.99730\teval-mlogloss:1.08726\n",
      "[166]\ttrain-mlogloss:0.99679\teval-mlogloss:1.08722\n",
      "[167]\ttrain-mlogloss:0.99620\teval-mlogloss:1.08712\n",
      "[168]\ttrain-mlogloss:0.99568\teval-mlogloss:1.08709\n",
      "[169]\ttrain-mlogloss:0.99516\teval-mlogloss:1.08708\n",
      "[170]\ttrain-mlogloss:0.99470\teval-mlogloss:1.08705\n",
      "[171]\ttrain-mlogloss:0.99398\teval-mlogloss:1.08699\n",
      "[172]\ttrain-mlogloss:0.99339\teval-mlogloss:1.08694\n",
      "[173]\ttrain-mlogloss:0.99290\teval-mlogloss:1.08693\n",
      "[174]\ttrain-mlogloss:0.99233\teval-mlogloss:1.08693\n",
      "[175]\ttrain-mlogloss:0.99186\teval-mlogloss:1.08691\n",
      "[176]\ttrain-mlogloss:0.99143\teval-mlogloss:1.08693\n",
      "[177]\ttrain-mlogloss:0.99082\teval-mlogloss:1.08695\n",
      "[178]\ttrain-mlogloss:0.99021\teval-mlogloss:1.08696\n",
      "[179]\ttrain-mlogloss:0.98966\teval-mlogloss:1.08696\n",
      "[180]\ttrain-mlogloss:0.98914\teval-mlogloss:1.08698\n",
      "[181]\ttrain-mlogloss:0.98851\teval-mlogloss:1.08695\n",
      "[182]\ttrain-mlogloss:0.98800\teval-mlogloss:1.08700\n",
      "[183]\ttrain-mlogloss:0.98743\teval-mlogloss:1.08698\n",
      "[184]\ttrain-mlogloss:0.98686\teval-mlogloss:1.08696\n",
      "[185]\ttrain-mlogloss:0.98634\teval-mlogloss:1.08694\n",
      "[186]\ttrain-mlogloss:0.98591\teval-mlogloss:1.08693\n",
      "[187]\ttrain-mlogloss:0.98534\teval-mlogloss:1.08687\n",
      "[188]\ttrain-mlogloss:0.98469\teval-mlogloss:1.08686\n",
      "[189]\ttrain-mlogloss:0.98421\teval-mlogloss:1.08685\n",
      "[190]\ttrain-mlogloss:0.98386\teval-mlogloss:1.08684\n",
      "[191]\ttrain-mlogloss:0.98325\teval-mlogloss:1.08673\n",
      "[192]\ttrain-mlogloss:0.98274\teval-mlogloss:1.08677\n",
      "[193]\ttrain-mlogloss:0.98223\teval-mlogloss:1.08678\n",
      "[194]\ttrain-mlogloss:0.98186\teval-mlogloss:1.08677\n",
      "[195]\ttrain-mlogloss:0.98131\teval-mlogloss:1.08676\n",
      "[196]\ttrain-mlogloss:0.98076\teval-mlogloss:1.08677\n",
      "[197]\ttrain-mlogloss:0.98018\teval-mlogloss:1.08670\n",
      "[198]\ttrain-mlogloss:0.97954\teval-mlogloss:1.08669\n",
      "[199]\ttrain-mlogloss:0.97902\teval-mlogloss:1.08673\n",
      "[200]\ttrain-mlogloss:0.97849\teval-mlogloss:1.08669\n",
      "[201]\ttrain-mlogloss:0.97798\teval-mlogloss:1.08674\n",
      "[202]\ttrain-mlogloss:0.97745\teval-mlogloss:1.08674\n",
      "[203]\ttrain-mlogloss:0.97701\teval-mlogloss:1.08672\n",
      "[204]\ttrain-mlogloss:0.97634\teval-mlogloss:1.08666\n",
      "[205]\ttrain-mlogloss:0.97586\teval-mlogloss:1.08670\n",
      "[206]\ttrain-mlogloss:0.97519\teval-mlogloss:1.08667\n",
      "[207]\ttrain-mlogloss:0.97473\teval-mlogloss:1.08669\n",
      "[208]\ttrain-mlogloss:0.97425\teval-mlogloss:1.08670\n",
      "[209]\ttrain-mlogloss:0.97373\teval-mlogloss:1.08676\n",
      "[210]\ttrain-mlogloss:0.97331\teval-mlogloss:1.08678\n",
      "[211]\ttrain-mlogloss:0.97278\teval-mlogloss:1.08676\n",
      "[212]\ttrain-mlogloss:0.97226\teval-mlogloss:1.08673\n",
      "[213]\ttrain-mlogloss:0.97172\teval-mlogloss:1.08671\n",
      "[214]\ttrain-mlogloss:0.97128\teval-mlogloss:1.08671\n",
      "[215]\ttrain-mlogloss:0.97083\teval-mlogloss:1.08671\n",
      "[216]\ttrain-mlogloss:0.97035\teval-mlogloss:1.08677\n",
      "[217]\ttrain-mlogloss:0.96994\teval-mlogloss:1.08681\n",
      "[218]\ttrain-mlogloss:0.96937\teval-mlogloss:1.08681\n",
      "[219]\ttrain-mlogloss:0.96904\teval-mlogloss:1.08679\n",
      "[220]\ttrain-mlogloss:0.96854\teval-mlogloss:1.08677\n",
      "[221]\ttrain-mlogloss:0.96806\teval-mlogloss:1.08682\n",
      "[222]\ttrain-mlogloss:0.96752\teval-mlogloss:1.08684\n",
      "[223]\ttrain-mlogloss:0.96703\teval-mlogloss:1.08685\n",
      "[224]\ttrain-mlogloss:0.96645\teval-mlogloss:1.08687\n",
      "Stopping. Best iteration:\n",
      "[204]\ttrain-mlogloss:0.97634\teval-mlogloss:1.08666\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGBoost 모델을 훈련 데이터로 학습한다!\n",
    "watch_list = [(dtrn, 'train'), (dvld, 'eval')]\n",
    "model = xgb.train(param, dtrn, num_boost_round=1000, evals=watch_list, early_stopping_rounds=20)\n",
    "\n",
    "# 코드 돌리는 데 많은 시간이 소요됨 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습한 모델을 저장한다.\n",
    "import pickle\n",
    "pickle.dump(model, open(\"../model/xgb.baseline.pkl\", \"wb\"))\n",
    "best_ntree_limit = model.best_ntree_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chanmi Yoo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Chanmi Yoo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Chanmi Yoo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# MAP@7 평가 척도를 위한 준비작업이다.\n",
    "# 고객 식별 번호를 추출한다.\n",
    "vld = trn[trn['fecha_dato'] == vld_date]\n",
    "ncodpers_vld = vld.as_matrix(columns=['ncodpers'])\n",
    "# 검증 데이터에서 신규 구매를 구한다.\n",
    "for prod in prods:\n",
    "    prev = prod + '_prev'\n",
    "    padd = prod + '_add'\n",
    "    vld[padd] = vld[prod] - vld[prev]    \n",
    "add_vld = vld.as_matrix(columns=[prod + '_add' for prod in prods])\n",
    "add_vld_list = [list() for i in range(len(ncodpers_vld))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고객별 신규 구매 정답 값을 add_vld_list에 저장하고, 총 count를 count_vld에 저장한다.\n",
    "count_vld = 0\n",
    "for ncodper in range(len(ncodpers_vld)):\n",
    "    for prod in range(len(prods)):\n",
    "        if add_vld[ncodper, prod] > 0:\n",
    "            add_vld_list[ncodper].append(prod)\n",
    "            count_vld += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mapk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-08dc5f1794f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 검증 데이터에서 얻을 수 있는 MAP@7 최고점을 미리 구한다. (0.042663)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madd_vld_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_vld_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'mapk' is not defined"
     ]
    }
   ],
   "source": [
    "# 검증 데이터에서 얻을 수 있는 MAP@7 최고점을 미리 구한다. (0.042663)\n",
    "print(mapk(add_vld_list, add_vld_list, 7, 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chanmi Yoo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "C:\\Users\\Chanmi Yoo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# 검증 데이터에 대한 예측 값을 구한다.\n",
    "X_vld = vld.as_matrix(columns=features)\n",
    "Y_vld = vld.as_matrix(columns=['y'])\n",
    "dvld = xgb.DMatrix(X_vld, label=Y_vld, feature_names=features)\n",
    "preds_vld = model.predict(dvld, ntree_limit=best_ntree_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chanmi Yoo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# 저번 달에 보유한 제품은 신규 구매가 불가하기 때문에, 확률값에서 미리 1을 빼준다\n",
    "preds_vld = preds_vld - vld.as_matrix(columns=[prod + '_prev' for prod in prods])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증 데이터 예측 상위 7개를 추출한다.\n",
    "result_vld = []\n",
    "for ncodper, pred in zip(ncodpers_vld, preds_vld):\n",
    "    y_prods = [(y,p,ip) for y,p,ip in zip(pred, prods, range(len(prods)))]\n",
    "    y_prods = sorted(y_prods, key=lambda a: a[0], reverse=True)[:7]\n",
    "    result_vld.append([ip for y,p,ip in y_prods])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증 데이터에서의 MAP@7 점수를 구한다. (0.036466)\n",
    "print(mapk(add_vld_list, result_vld, 7, 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chanmi Yoo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "C:\\Users\\Chanmi Yoo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.72653\n",
      "[1]\ttrain-mlogloss:2.45788\n",
      "[2]\ttrain-mlogloss:2.27489\n",
      "[3]\ttrain-mlogloss:2.13560\n",
      "[4]\ttrain-mlogloss:2.02158\n",
      "[5]\ttrain-mlogloss:1.92519\n",
      "[6]\ttrain-mlogloss:1.84686\n",
      "[7]\ttrain-mlogloss:1.77856\n",
      "[8]\ttrain-mlogloss:1.71674\n",
      "[9]\ttrain-mlogloss:1.66394\n",
      "[10]\ttrain-mlogloss:1.61715\n",
      "[11]\ttrain-mlogloss:1.57391\n",
      "[12]\ttrain-mlogloss:1.53472\n",
      "[13]\ttrain-mlogloss:1.50084\n",
      "[14]\ttrain-mlogloss:1.47000\n",
      "[15]\ttrain-mlogloss:1.44046\n",
      "[16]\ttrain-mlogloss:1.41425\n",
      "[17]\ttrain-mlogloss:1.39089\n",
      "[18]\ttrain-mlogloss:1.36909\n",
      "[19]\ttrain-mlogloss:1.34917\n",
      "[20]\ttrain-mlogloss:1.33089\n",
      "[21]\ttrain-mlogloss:1.31322\n",
      "[22]\ttrain-mlogloss:1.29747\n",
      "[23]\ttrain-mlogloss:1.28293\n",
      "[24]\ttrain-mlogloss:1.26957\n",
      "[25]\ttrain-mlogloss:1.25662\n",
      "[26]\ttrain-mlogloss:1.24471\n",
      "[27]\ttrain-mlogloss:1.23375\n",
      "[28]\ttrain-mlogloss:1.22353\n",
      "[29]\ttrain-mlogloss:1.21446\n",
      "[30]\ttrain-mlogloss:1.20597\n",
      "[31]\ttrain-mlogloss:1.19733\n",
      "[32]\ttrain-mlogloss:1.18932\n",
      "[33]\ttrain-mlogloss:1.18198\n",
      "[34]\ttrain-mlogloss:1.17500\n",
      "[35]\ttrain-mlogloss:1.16834\n",
      "[36]\ttrain-mlogloss:1.16211\n",
      "[37]\ttrain-mlogloss:1.15623\n",
      "[38]\ttrain-mlogloss:1.15090\n",
      "[39]\ttrain-mlogloss:1.14574\n",
      "[40]\ttrain-mlogloss:1.14098\n",
      "[41]\ttrain-mlogloss:1.13641\n",
      "[42]\ttrain-mlogloss:1.13211\n",
      "[43]\ttrain-mlogloss:1.12793\n",
      "[44]\ttrain-mlogloss:1.12392\n",
      "[45]\ttrain-mlogloss:1.12029\n",
      "[46]\ttrain-mlogloss:1.11682\n",
      "[47]\ttrain-mlogloss:1.11336\n",
      "[48]\ttrain-mlogloss:1.11026\n",
      "[49]\ttrain-mlogloss:1.10716\n",
      "[50]\ttrain-mlogloss:1.10423\n",
      "[51]\ttrain-mlogloss:1.10134\n",
      "[52]\ttrain-mlogloss:1.09871\n",
      "[53]\ttrain-mlogloss:1.09614\n",
      "[54]\ttrain-mlogloss:1.09377\n",
      "[55]\ttrain-mlogloss:1.09142\n",
      "[56]\ttrain-mlogloss:1.08933\n",
      "[57]\ttrain-mlogloss:1.08732\n",
      "[58]\ttrain-mlogloss:1.08538\n",
      "[59]\ttrain-mlogloss:1.08341\n",
      "[60]\ttrain-mlogloss:1.08149\n",
      "[61]\ttrain-mlogloss:1.07983\n",
      "[62]\ttrain-mlogloss:1.07810\n",
      "[63]\ttrain-mlogloss:1.07654\n",
      "[64]\ttrain-mlogloss:1.07499\n",
      "[65]\ttrain-mlogloss:1.07351\n",
      "[66]\ttrain-mlogloss:1.07213\n",
      "[67]\ttrain-mlogloss:1.07077\n",
      "[68]\ttrain-mlogloss:1.06938\n",
      "[69]\ttrain-mlogloss:1.06801\n",
      "[70]\ttrain-mlogloss:1.06668\n",
      "[71]\ttrain-mlogloss:1.06550\n",
      "[72]\ttrain-mlogloss:1.06437\n",
      "[73]\ttrain-mlogloss:1.06317\n",
      "[74]\ttrain-mlogloss:1.06191\n",
      "[75]\ttrain-mlogloss:1.06092\n",
      "[76]\ttrain-mlogloss:1.05980\n",
      "[77]\ttrain-mlogloss:1.05877\n",
      "[78]\ttrain-mlogloss:1.05785\n",
      "[79]\ttrain-mlogloss:1.05688\n",
      "[80]\ttrain-mlogloss:1.05591\n",
      "[81]\ttrain-mlogloss:1.05497\n",
      "[82]\ttrain-mlogloss:1.05403\n",
      "[83]\ttrain-mlogloss:1.05319\n",
      "[84]\ttrain-mlogloss:1.05238\n",
      "[85]\ttrain-mlogloss:1.05144\n",
      "[86]\ttrain-mlogloss:1.05058\n",
      "[87]\ttrain-mlogloss:1.04964\n",
      "[88]\ttrain-mlogloss:1.04883\n",
      "[89]\ttrain-mlogloss:1.04808\n",
      "[90]\ttrain-mlogloss:1.04737\n",
      "[91]\ttrain-mlogloss:1.04665\n",
      "[92]\ttrain-mlogloss:1.04591\n",
      "[93]\ttrain-mlogloss:1.04521\n",
      "[94]\ttrain-mlogloss:1.04447\n",
      "[95]\ttrain-mlogloss:1.04385\n",
      "[96]\ttrain-mlogloss:1.04332\n",
      "[97]\ttrain-mlogloss:1.04269\n",
      "[98]\ttrain-mlogloss:1.04204\n",
      "[99]\ttrain-mlogloss:1.04146\n",
      "[100]\ttrain-mlogloss:1.04083\n",
      "[101]\ttrain-mlogloss:1.04026\n",
      "[102]\ttrain-mlogloss:1.03954\n",
      "[103]\ttrain-mlogloss:1.03894\n",
      "[104]\ttrain-mlogloss:1.03835\n",
      "[105]\ttrain-mlogloss:1.03762\n",
      "[106]\ttrain-mlogloss:1.03705\n",
      "[107]\ttrain-mlogloss:1.03652\n",
      "[108]\ttrain-mlogloss:1.03590\n",
      "[109]\ttrain-mlogloss:1.03523\n",
      "[110]\ttrain-mlogloss:1.03476\n",
      "[111]\ttrain-mlogloss:1.03433\n",
      "[112]\ttrain-mlogloss:1.03377\n",
      "[113]\ttrain-mlogloss:1.03333\n",
      "[114]\ttrain-mlogloss:1.03274\n",
      "[115]\ttrain-mlogloss:1.03224\n",
      "[116]\ttrain-mlogloss:1.03170\n",
      "[117]\ttrain-mlogloss:1.03116\n",
      "[118]\ttrain-mlogloss:1.03072\n",
      "[119]\ttrain-mlogloss:1.03021\n",
      "[120]\ttrain-mlogloss:1.02971\n",
      "[121]\ttrain-mlogloss:1.02903\n",
      "[122]\ttrain-mlogloss:1.02852\n",
      "[123]\ttrain-mlogloss:1.02805\n",
      "[124]\ttrain-mlogloss:1.02758\n",
      "[125]\ttrain-mlogloss:1.02709\n",
      "[126]\ttrain-mlogloss:1.02660\n",
      "[127]\ttrain-mlogloss:1.02609\n",
      "[128]\ttrain-mlogloss:1.02564\n",
      "[129]\ttrain-mlogloss:1.02497\n",
      "[130]\ttrain-mlogloss:1.02448\n",
      "[131]\ttrain-mlogloss:1.02397\n",
      "[132]\ttrain-mlogloss:1.02348\n",
      "[133]\ttrain-mlogloss:1.02305\n",
      "[134]\ttrain-mlogloss:1.02250\n",
      "[135]\ttrain-mlogloss:1.02211\n",
      "[136]\ttrain-mlogloss:1.02156\n",
      "[137]\ttrain-mlogloss:1.02099\n",
      "[138]\ttrain-mlogloss:1.02040\n",
      "[139]\ttrain-mlogloss:1.01993\n",
      "[140]\ttrain-mlogloss:1.01952\n",
      "[141]\ttrain-mlogloss:1.01907\n",
      "[142]\ttrain-mlogloss:1.01860\n",
      "[143]\ttrain-mlogloss:1.01809\n",
      "[144]\ttrain-mlogloss:1.01760\n",
      "[145]\ttrain-mlogloss:1.01721\n",
      "[146]\ttrain-mlogloss:1.01678\n",
      "[147]\ttrain-mlogloss:1.01633\n",
      "[148]\ttrain-mlogloss:1.01589\n",
      "[149]\ttrain-mlogloss:1.01546\n",
      "[150]\ttrain-mlogloss:1.01496\n",
      "[151]\ttrain-mlogloss:1.01440\n",
      "[152]\ttrain-mlogloss:1.01389\n",
      "[153]\ttrain-mlogloss:1.01337\n",
      "[154]\ttrain-mlogloss:1.01282\n",
      "[155]\ttrain-mlogloss:1.01229\n",
      "[156]\ttrain-mlogloss:1.01178\n",
      "[157]\ttrain-mlogloss:1.01138\n",
      "[158]\ttrain-mlogloss:1.01084\n",
      "[159]\ttrain-mlogloss:1.01044\n",
      "[160]\ttrain-mlogloss:1.01006\n",
      "[161]\ttrain-mlogloss:1.00964\n",
      "[162]\ttrain-mlogloss:1.00917\n",
      "[163]\ttrain-mlogloss:1.00863\n",
      "[164]\ttrain-mlogloss:1.00819\n",
      "[165]\ttrain-mlogloss:1.00778\n",
      "[166]\ttrain-mlogloss:1.00719\n",
      "[167]\ttrain-mlogloss:1.00669\n",
      "[168]\ttrain-mlogloss:1.00619\n",
      "[169]\ttrain-mlogloss:1.00570\n",
      "[170]\ttrain-mlogloss:1.00524\n",
      "[171]\ttrain-mlogloss:1.00479\n",
      "[172]\ttrain-mlogloss:1.00422\n",
      "[173]\ttrain-mlogloss:1.00379\n",
      "[174]\ttrain-mlogloss:1.00326\n",
      "[175]\ttrain-mlogloss:1.00262\n",
      "[176]\ttrain-mlogloss:1.00216\n",
      "[177]\ttrain-mlogloss:1.00159\n",
      "[178]\ttrain-mlogloss:1.00108\n",
      "[179]\ttrain-mlogloss:1.00073\n",
      "[180]\ttrain-mlogloss:1.00033\n",
      "[181]\ttrain-mlogloss:0.99985\n",
      "[182]\ttrain-mlogloss:0.99941\n",
      "[183]\ttrain-mlogloss:0.99886\n",
      "[184]\ttrain-mlogloss:0.99825\n",
      "[185]\ttrain-mlogloss:0.99781\n",
      "[186]\ttrain-mlogloss:0.99729\n",
      "[187]\ttrain-mlogloss:0.99686\n",
      "[188]\ttrain-mlogloss:0.99624\n",
      "[189]\ttrain-mlogloss:0.99569\n",
      "[190]\ttrain-mlogloss:0.99524\n",
      "[191]\ttrain-mlogloss:0.99471\n",
      "[192]\ttrain-mlogloss:0.99431\n",
      "[193]\ttrain-mlogloss:0.99390\n",
      "[194]\ttrain-mlogloss:0.99336\n",
      "[195]\ttrain-mlogloss:0.99289\n",
      "[196]\ttrain-mlogloss:0.99253\n",
      "[197]\ttrain-mlogloss:0.99194\n",
      "[198]\ttrain-mlogloss:0.99151\n",
      "[199]\ttrain-mlogloss:0.99102\n",
      "[200]\ttrain-mlogloss:0.99054\n",
      "[201]\ttrain-mlogloss:0.99004\n",
      "[202]\ttrain-mlogloss:0.98950\n",
      "[203]\ttrain-mlogloss:0.98899\n",
      "[204]\ttrain-mlogloss:0.98851\n",
      "[205]\ttrain-mlogloss:0.98809\n",
      "[206]\ttrain-mlogloss:0.98763\n",
      "[207]\ttrain-mlogloss:0.98718\n",
      "[208]\ttrain-mlogloss:0.98667\n",
      "[209]\ttrain-mlogloss:0.98613\n",
      "[210]\ttrain-mlogloss:0.98567\n",
      "[211]\ttrain-mlogloss:0.98515\n",
      "[212]\ttrain-mlogloss:0.98474\n",
      "[213]\ttrain-mlogloss:0.98418\n",
      "[214]\ttrain-mlogloss:0.98377\n",
      "[215]\ttrain-mlogloss:0.98332\n",
      "[216]\ttrain-mlogloss:0.98293\n",
      "[217]\ttrain-mlogloss:0.98239\n",
      "[218]\ttrain-mlogloss:0.98194\n",
      "[219]\ttrain-mlogloss:0.98147\n",
      "[220]\ttrain-mlogloss:0.98105\n",
      "[221]\ttrain-mlogloss:0.98067\n",
      "[222]\ttrain-mlogloss:0.98025\n",
      "[223]\ttrain-mlogloss:0.97984\n",
      "[224]\ttrain-mlogloss:0.97915\n",
      "[225]\ttrain-mlogloss:0.97870\n",
      "[226]\ttrain-mlogloss:0.97825\n",
      "[227]\ttrain-mlogloss:0.97791\n",
      "[228]\ttrain-mlogloss:0.97735\n",
      "[229]\ttrain-mlogloss:0.97686\n",
      "[230]\ttrain-mlogloss:0.97625\n",
      "[231]\ttrain-mlogloss:0.97582\n",
      "[232]\ttrain-mlogloss:0.97535\n",
      "[233]\ttrain-mlogloss:0.97487\n",
      "[234]\ttrain-mlogloss:0.97442\n",
      "[235]\ttrain-mlogloss:0.97406\n",
      "[236]\ttrain-mlogloss:0.97353\n",
      "[237]\ttrain-mlogloss:0.97307\n",
      "[238]\ttrain-mlogloss:0.97266\n",
      "[239]\ttrain-mlogloss:0.97225\n",
      "[240]\ttrain-mlogloss:0.97183\n",
      "[241]\ttrain-mlogloss:0.97137\n",
      "[242]\ttrain-mlogloss:0.97100\n",
      "[243]\ttrain-mlogloss:0.97059\n",
      "[244]\ttrain-mlogloss:0.97020\n",
      "[245]\ttrain-mlogloss:0.96986\n",
      "[246]\ttrain-mlogloss:0.96952\n",
      "[247]\ttrain-mlogloss:0.96916\n",
      "[248]\ttrain-mlogloss:0.96863\n",
      "[249]\ttrain-mlogloss:0.96824\n",
      "[250]\ttrain-mlogloss:0.96789\n",
      "[251]\ttrain-mlogloss:0.96754\n",
      "[252]\ttrain-mlogloss:0.96712\n"
     ]
    }
   ],
   "source": [
    "# XGBoost 모델을 전체 훈련 데이터로 재학습한다!\n",
    "X_all = XY.as_matrix(columns=features)\n",
    "Y_all = XY.as_matrix(columns=['y'])\n",
    "dall = xgb.DMatrix(X_all, label=Y_all, feature_names=features)\n",
    "watch_list = [(dall, 'train')]\n",
    "# 트리 개수를 늘어난 데이터 양만큼 비례해서 증가한다.\n",
    "best_ntree_limit = int(best_ntree_limit * (len(XY_trn) + len(XY_vld)) / len(XY_trn))\n",
    "# XGBoost 모델 재학습!\n",
    "model = xgb.train(param, dall, num_boost_round=best_ntree_limit, evals=watch_list)\n",
    "\n",
    "# 코드 돌리는 데 많은 시간이 소요됨 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance:\n",
      "('renta', 20758)\n",
      "('age', 20117)\n",
      "('antiguedad', 18435)\n",
      "('age_prev', 13222)\n",
      "('antiguedad_prev', 13090)\n",
      "('fecha_alta_month', 12186)\n",
      "('nomprov', 11558)\n",
      "('fecha_alta_year', 9713)\n",
      "('renta_prev', 9031)\n",
      "('canal_entrada', 7811)\n",
      "('nomprov_prev', 6661)\n",
      "('canal_entrada_prev', 4819)\n",
      "('fecha_alta_month_prev', 4503)\n",
      "('ind_recibo_ult1_prev', 3417)\n",
      "('sexo', 3406)\n",
      "('fecha_alta_year_prev', 3336)\n",
      "('ind_ecue_fin_ult1_prev', 3125)\n",
      "('ind_cco_fin_ult1_prev', 3071)\n",
      "('ind_cno_fin_ult1_prev', 2980)\n",
      "('segmento', 2250)\n",
      "('ind_tjcr_fin_ult1_prev', 2173)\n",
      "('ind_reca_fin_ult1_prev', 2095)\n",
      "('segmento_prev', 1962)\n",
      "('ind_nom_pens_ult1_prev', 1698)\n",
      "('tiprel_1mes', 1669)\n",
      "('ind_valo_fin_ult1_prev', 1635)\n",
      "('ind_dela_fin_ult1_prev', 1549)\n",
      "('ind_ctop_fin_ult1_prev', 1494)\n",
      "('ind_nomina_ult1_prev', 1471)\n",
      "('ind_actividad_cliente', 1331)\n",
      "('sexo_prev', 1303)\n",
      "('ind_ctpp_fin_ult1_prev', 1178)\n",
      "('tiprel_1mes_prev', 1160)\n",
      "('ind_fond_fin_ult1_prev', 1011)\n",
      "('ind_ctma_fin_ult1_prev', 922)\n",
      "('ind_actividad_cliente_prev', 912)\n",
      "('indext', 752)\n",
      "('ind_nuevo', 722)\n",
      "('ind_plan_fin_ult1_prev', 657)\n",
      "('ind_hip_fin_ult1_prev', 519)\n",
      "('ind_nuevo_prev', 403)\n",
      "('ind_deco_fin_ult1_prev', 395)\n",
      "('indext_prev', 393)\n",
      "('indrel_1mes', 392)\n",
      "('pais_residencia', 239)\n",
      "('ind_empleado_prev', 193)\n",
      "('indrel_1mes_prev', 190)\n",
      "('ind_viv_fin_ult1_prev', 183)\n",
      "('indrel', 159)\n",
      "('ind_empleado', 149)\n",
      "('pais_residencia_prev', 130)\n",
      "('ind_deme_fin_ult1_prev', 110)\n",
      "('ind_ctju_fin_ult1_prev', 105)\n",
      "('ind_pres_fin_ult1_prev', 84)\n",
      "('ult_fec_cli_1t_month', 63)\n",
      "('ind_cder_fin_ult1_prev', 40)\n",
      "('indfall', 37)\n",
      "('indfall_prev', 26)\n",
      "('conyuemp_prev', 21)\n",
      "('indresi', 16)\n",
      "('ult_fec_cli_1t_year', 13)\n",
      "('conyuemp', 9)\n",
      "('indresi_prev', 5)\n"
     ]
    }
   ],
   "source": [
    "# 변수 중요도를 출력해본다. 예상하던 변수가 상위로 올라와 있는가?\n",
    "print(\"Feature importance:\")\n",
    "for kv in sorted([(k,v) for k,v in model.get_fscore().items()], key=lambda kv: kv[1], reverse=True):\n",
    "    print(kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chanmi Yoo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "C:\\Users\\Chanmi Yoo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"\n",
      "C:\\Users\\Chanmi Yoo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# 캐글 제출을 위하여 테스트 데이터에 대한 예측 값을 구한다.\n",
    "X_tst = tst.as_matrix(columns=features)\n",
    "dtst = xgb.DMatrix(X_tst, feature_names=features)\n",
    "preds_tst = model.predict(dtst, ntree_limit=best_ntree_limit)\n",
    "ncodpers_tst = tst.as_matrix(columns=['ncodpers'])\n",
    "preds_tst = preds_tst - tst.as_matrix(columns=[prod + '_prev' for prod in prods])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출 파일을 생성한다.\n",
    "submit_file = open('../model/xgb.baseline.2015-06-28', 'w')\n",
    "submit_file.write('ncodpers,added_products\\n')\n",
    "for ncodper, pred in zip(ncodpers_tst, preds_tst):\n",
    "    y_prods = [(y,p,ip) for y,p,ip in zip(pred, prods, range(len(prods)))]\n",
    "    y_prods = sorted(y_prods, key=lambda a: a[0], reverse=True)[:7]\n",
    "    y_prods = [p for y,p,ip in y_prods]\n",
    "    submit_file.write('{},{}\\n'.format(int(ncodper), ' '.join(y_prods)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_trn이 얼마나 대용량인지 참고 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 12020685 entries, 0 to 12020684\n",
      "Columns: 104 entries, fecha_dato to ult_fec_cli_1t_year_prev\n",
      "dtypes: float64(52), int16(2), int64(37), int8(7), object(6)\n",
      "memory usage: 8.7+ GB\n"
     ]
    }
   ],
   "source": [
    "df_trn.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
